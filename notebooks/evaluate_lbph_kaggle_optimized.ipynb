{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# LBPH Evaluation - Kaggle (Optimized)\n",
    "# \n",
    "# Phi\u00ean b\u1ea3n t\u1ed1i \u01b0u v\u1edbi:\n",
    "# - Gi\u1edbi h\u1ea1n max 3 \u1ea3nh/identity \u0111\u1ec3 gi\u1ea3m th\u1eddi gian t\u1eeb ~10h xu\u1ed1ng v\u00e0i ph\u00fat\n",
    "# - L\u01b0u intermediate results \u0111\u1ec3 ph\u1ee5c h\u1ed3i khi kernel restart\n",
    "# - File zip \u0111\u1ea7y \u0111\u1ee7 d\u1eef li\u1ec7u \u0111\u1ec3 tr\u1ef1c quan h\u00f3a\n",
    "# ---\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBPH Evaluation - Kaggle (Optimized)\n",
    "\n",
    "Evaluation notebook cho LBPH model s\u1eed d\u1ee5ng threshold-based classification.\n",
    "\n",
    "## Optimization:\n",
    "- **MAX_IMAGES_PER_IDENTITY = 3**: Gi\u1ea3m th\u1eddi gian \u0111\u00e1nh gi\u00e1 t\u1eeb ~10 ti\u1ebfng xu\u1ed1ng v\u00e0i ph\u00fat\n",
    "- **Intermediate logging**: L\u01b0u k\u1ebft qu\u1ea3 sau m\u1ed7i b\u01b0\u1edbc \u0111\u1ec3 ph\u1ee5c h\u1ed3i khi session crash\n",
    "- **Extended zip file**: Bao g\u1ed3m logs chi ti\u1ebft \u0111\u1ec3 visualize offline\n",
    "\n",
    "## Approach:\n",
    "- Load LBPH model t\u1eeb checkpoint (XML file)\n",
    "- Load validation v\u00e0 test datasets (gi\u1edbi h\u1ea1n s\u1ed1 \u1ea3nh)\n",
    "- T\u00ecm threshold t\u1ed1i \u01b0u tr\u00ean validation set\n",
    "- Evaluate tr\u00ean test set v\u1edbi threshold \u0111\u00e3 ch\u1ecdn\n",
    "- Metrics: Accuracy, Coverage, Threshold analysis\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, json\n",
    "import shutil, glob\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "ROOT = \"/kaggle/working/FaceRecognition\"\n",
    "CHECKPOINT_DIR = \"/kaggle/working/checkpoints/lbph\"\n",
    "KAGGLE_DATASET_NAME = \"celeba-aligned-balanced\"\n",
    "DATA_DIR = f\"/kaggle/input/{KAGGLE_DATASET_NAME}\"\n",
    "CHECKPOINT_DATASET_NAME = \"lbph-checkpoints\"\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "LOG_DIR = f\"{OUTPUT_DIR}/logs/lbph\"\n",
    "\n",
    "# Gi\u1edbi h\u1ea1n s\u1ed1 \u1ea3nh m\u1ed7i identity \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 nhanh h\u01a1n\n",
    "# V\u1edbi 200 identities * 3 \u1ea3nh = 600 samples, \u0111\u1ee7 \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 ch\u00ednh x\u00e1c\n",
    "# thay v\u00ec load to\u00e0n b\u1ed9 ~2000+ \u1ea3nh (c\u00f3 th\u1ec3 m\u1ea5t 10+ ti\u1ebfng)\n",
    "MAX_IMAGES_PER_IDENTITY = 3\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"[CONFIG] MAX_IMAGES_PER_IDENTITY = {MAX_IMAGES_PER_IDENTITY}\")\n",
    "print(f\"[CONFIG] LOG_DIR = {LOG_DIR}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy checkpoint t\u1eeb Kaggle input dataset (n\u1ebfu c\u00f3)\n",
    "checkpoint_input = f\"/kaggle/input/{CHECKPOINT_DATASET_NAME}\"\n",
    "checkpoint_found = False\n",
    "\n",
    "if os.path.exists(checkpoint_input):\n",
    "    for xml_file in glob.glob(os.path.join(checkpoint_input, \"**/*.xml\"), recursive=True):\n",
    "        dest = os.path.join(CHECKPOINT_DIR, os.path.basename(xml_file))\n",
    "        if not os.path.exists(dest) or os.path.getsize(dest) != os.path.getsize(xml_file):\n",
    "            print(f\"Copying {os.path.basename(xml_file)} from Kaggle input...\")\n",
    "            shutil.copy(xml_file, dest)\n",
    "            if os.path.getsize(dest) == os.path.getsize(xml_file):\n",
    "                print(f\"  [OK] Copied successfully\")\n",
    "                checkpoint_found = True\n",
    "            else:\n",
    "                print(f\"  [WARNING] Size mismatch after copy!\")\n",
    "        else:\n",
    "            checkpoint_found = True\n",
    "    if checkpoint_found:\n",
    "        print(f\"Checkpoints from Kaggle input: {os.listdir(CHECKPOINT_DIR)}\")\n",
    "else:\n",
    "    print(f\"[INFO] Kh\u00f4ng t\u00ecm th\u1ea5y checkpoint trong Kaggle input: {checkpoint_input}\")\n",
    "    print(f\"      S\u1ebd th\u1eed copy t\u1eeb repo sau khi clone...\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cau hinh GitHub token\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "    print(\"[OK] Da lay GITHUB_TOKEN\")\n",
    "except Exception as e:\n",
    "    GITHUB_TOKEN = None\n",
    "    print(\"[INFO] Su dung public URL\")\n",
    "\n",
    "if GITHUB_TOKEN:\n",
    "    REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/sin0235/FaceRecognition.git\"\n",
    "else:\n",
    "    REPO_URL = \"https://github.com/sin0235/FaceRecognition.git\"\n",
    "\n",
    "# Clone repository\n",
    "if os.path.exists(ROOT):\n",
    "    print(\"Repository da ton tai, dang pull updates...\")\n",
    "    os.chdir(ROOT)\n",
    "    if GITHUB_TOKEN:\n",
    "        os.system(f\"git remote set-url origin {REPO_URL}\")\n",
    "    os.system(\"git pull --no-rebase origin fix/lbph-module\")\n",
    "else:\n",
    "    print(f\"Dang clone repository...\")\n",
    "    os.system(f\"git clone {REPO_URL} {ROOT}\")\n",
    "    os.chdir(ROOT)\n",
    "\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
    "\n",
    "# Th\u00eam ROOT v\u00e0o sys.path \u0111\u1ec3 import modules\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "    print(f\"\\n[OK] Added {ROOT} to sys.path\")\n",
    "\n",
    "# Copy checkpoint t\u1eeb repo n\u1ebfu ch\u01b0a c\u00f3 t\u1eeb Kaggle input\n",
    "model_path_check = os.path.join(CHECKPOINT_DIR, \"lbph_model.xml\")\n",
    "if not os.path.exists(model_path_check):\n",
    "    repo_checkpoint_path = os.path.join(ROOT, \"models\", \"checkpoints\", \"LBHP\", \"lbph_model.xml\")\n",
    "    if os.path.exists(repo_checkpoint_path):\n",
    "        print(f\"\\nCopying checkpoint from repo: {repo_checkpoint_path}\")\n",
    "        shutil.copy(repo_checkpoint_path, model_path_check)\n",
    "        if os.path.exists(model_path_check):\n",
    "            file_size = os.path.getsize(model_path_check)\n",
    "            print(f\"  [OK] Copied successfully ({file_size / 1024 / 1024:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"  [ERROR] Failed to copy checkpoint\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Kh\u00f4ng t\u00ecm th\u1ea5y checkpoint trong repo: {repo_checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"\\n[OK] Checkpoint \u0111\u00e3 c\u00f3 s\u1eb5n: {model_path_check}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C\u00e0i \u0111\u1eb7t dependencies v\u1edbi th\u1ee9 t\u1ef1 \u0111\u00fang \u0111\u1ec3 tr\u00e1nh xung \u0111\u1ed9t\n",
    "os.system(\"pip install -q --upgrade numpy\")\n",
    "os.system(\"pip install -q --upgrade scipy\")\n",
    "os.system(\"pip install -q --upgrade --force-reinstall scikit-learn\")\n",
    "os.system(\"pip install -q opencv-python-headless opencv-contrib-python-headless Pillow tqdm pyyaml matplotlib seaborn\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorBoard warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import c\u00e1c th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "    print(\"[OK] All imports successful\")\n",
    "except (ImportError, AttributeError, TypeError, ValueError) as e:\n",
    "    print(f\"\u0110ang fix compatibility: {e}\")\n",
    "    print(\"Reinstalling numpy, scipy, scikit-learn, matplotlib, seaborn...\")\n",
    "    os.system(\"pip install -q --upgrade --force-reinstall numpy scipy scikit-learn matplotlib seaborn\")\n",
    "    \n",
    "    # X\u00f3a T\u1ea4T C\u1ea2 modules li\u00ean quan \u0111\u1ec3 import l\u1ea1i t\u1eeb \u0111\u1ea7u\n",
    "    modules_to_remove = [k for k in list(sys.modules.keys()) \n",
    "                        if any(x in k for x in ['numpy', 'matplotlib', 'seaborn', 'scipy', 'sklearn', 'pandas'])]\n",
    "    for mod in modules_to_remove:\n",
    "        try:\n",
    "            del sys.modules[mod]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Import l\u1ea1i t\u1eeb \u0111\u1ea7u\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "    print(\"[OK] All imports successful after fix\")\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Model\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(CHECKPOINT_DIR, \"lbph_model.xml\")\n",
    "\n",
    "# Validate checkpoint file tr\u01b0\u1edbc khi load\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Model kh\u00f4ng t\u1ed3n t\u1ea1i: {model_path}\\n\"\n",
    "        f\"Vui l\u00f2ng ki\u1ec3m tra:\"\n",
    "        f\"  1. Dataset checkpoint \u0111\u00e3 \u0111\u01b0\u1ee3c add v\u00e0o Kaggle input ch\u01b0a?\"\n",
    "        f\"  2. T\u00ean dataset c\u00f3 \u0111\u00fang '{CHECKPOINT_DATASET_NAME}' kh\u00f4ng?\"\n",
    "        f\"  3. File lbph_model.xml c\u00f3 trong dataset kh\u00f4ng?\"\n",
    "    )\n",
    "\n",
    "file_size = os.path.getsize(model_path)\n",
    "print(f\"Model file: {model_path}\")\n",
    "print(f\"File size: {file_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "if file_size < 1024:\n",
    "    raise ValueError(f\"Model file qu\u00e1 nh\u1ecf ({file_size} bytes), c\u00f3 th\u1ec3 b\u1ecb h\u1ecfng\")\n",
    "\n",
    "# Load LBPH model\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "model.read(model_path)\n",
    "\n",
    "print(f\"\\n[OK] LBPH model loaded successfully\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Data (v\u1edbi gi\u1edbi h\u1ea1n s\u1ed1 \u1ea3nh)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lbphmodel.dataset_lbph import load_data_no_haar\n",
    "\n",
    "# T\u00ecm data dirs\n",
    "train_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"train\")\n",
    "val_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"val\")\n",
    "test_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"test\")\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    train_dir = os.path.join(DATA_DIR, \"train\")\n",
    "    val_dir = os.path.join(DATA_DIR, \"val\")\n",
    "    test_dir = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "print(f\"Train dir: {train_dir}\")\n",
    "print(f\"Val dir: {val_dir}\")\n",
    "print(f\"Test dir: {test_dir}\")\n",
    "print(f\"Max images per identity: {MAX_IMAGES_PER_IDENTITY}\")\n",
    "\n",
    "# Load validation v\u00e0 test data v\u1edbi gi\u1edbi h\u1ea1n s\u1ed1 \u1ea3nh\n",
    "# Gi\u00fap gi\u1ea3m th\u1eddi gian \u0111\u00e1nh gi\u00e1 t\u1eeb ~10 ti\u1ebfng xu\u1ed1ng v\u00e0i ph\u00fat\n",
    "val_faces, val_labels = load_data_no_haar(val_dir, max_images_per_identity=MAX_IMAGES_PER_IDENTITY)\n",
    "test_faces, test_labels = load_data_no_haar(test_dir, max_images_per_identity=MAX_IMAGES_PER_IDENTITY)\n",
    "\n",
    "print(f\"\\nVal samples: {len(val_faces)}\")\n",
    "print(f\"Test samples: {len(test_faces)}\")\n",
    "print(f\"Val labels range: {val_labels.min()} - {val_labels.max()}\")\n",
    "print(f\"Test labels range: {test_labels.min()} - {test_labels.max()}\")\n",
    "\n",
    "# L\u01b0u th\u00f4ng tin data load \u0111\u1ec3 ph\u1ee5c h\u1ed3i\n",
    "data_info = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'max_images_per_identity': MAX_IMAGES_PER_IDENTITY,\n",
    "    'val_samples': len(val_faces),\n",
    "    'test_samples': len(test_faces),\n",
    "    'val_identities': len(set(val_labels)),\n",
    "    'test_identities': len(set(test_labels))\n",
    "}\n",
    "with open(f'{LOG_DIR}/data_info.json', 'w') as f:\n",
    "    json.dump(data_info, f, indent=2)\n",
    "print(f\"\\n[OK] Data info saved to {LOG_DIR}/data_info.json\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Find Optimal Threshold\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lbphmodel.threshold_lbph import find_optimal_threshold\n",
    "\n",
    "# T\u00ecm threshold t\u1ed1i \u01b0u tr\u00ean validation set\n",
    "print(\"Finding optimal threshold on validation set...\")\n",
    "start_time = time.time()\n",
    "best_threshold, best_score, threshold_results = find_optimal_threshold(\n",
    "    model, val_faces, val_labels, min_coverage=0.3\n",
    ")\n",
    "threshold_search_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"OPTIMAL THRESHOLD\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Search time: {threshold_search_time:.2f}s ({threshold_search_time/60:.2f} min)\")\n",
    "print(f\"Best threshold: {best_threshold}\")\n",
    "print(f\"Best score (acc * coverage): {best_score:.4f}\")\n",
    "print(f\"\\nTop 5 thresholds:\")\n",
    "for thr, acc, cov, score in sorted(threshold_results, key=lambda x: x[3], reverse=True)[:5]:\n",
    "    print(f\"  Threshold={thr:3d}: Accuracy={acc:.3f}, Coverage={cov:.3f}, Score={score:.4f}\")\n",
    "\n",
    "# L\u01b0u k\u1ebft qu\u1ea3 threshold ngay l\u1eadp t\u1ee9c \u0111\u1ec3 tr\u00e1nh m\u1ea5t d\u1eef li\u1ec7u\n",
    "threshold_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'search_time_seconds': threshold_search_time,\n",
    "    'best_threshold': int(best_threshold),\n",
    "    'best_score': float(best_score),\n",
    "    'all_results': [{'threshold': int(t), 'accuracy': float(a), 'coverage': float(c), 'score': float(s)} for t, a, c, s in threshold_results]\n",
    "}\n",
    "with open(f'{LOG_DIR}/threshold_search.json', 'w') as f:\n",
    "    json.dump(threshold_data, f, indent=2)\n",
    "print(f\"\\n[OK] Threshold results saved to {LOG_DIR}/threshold_search.json\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Evaluate on Test Set\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lbphmodel.evaluate_lbph import evaluate_lbph\n",
    "\n",
    "# Evaluate tr\u00ean test set v\u1edbi threshold \u0111\u00e3 ch\u1ecdn\n",
    "start_time = time.time()\n",
    "test_acc, test_cov, test_used, test_confidences = evaluate_lbph(\n",
    "    model, test_faces, test_labels, best_threshold\n",
    ")\n",
    "eval_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TEST SET EVALUATION\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Evaluation time: {eval_time:.2f}s ({eval_time/60:.2f} min)\")\n",
    "print(f\"Threshold: {best_threshold}\")\n",
    "print(f\"Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"Coverage: {test_cov:.4f} ({test_cov*100:.2f}%)\")\n",
    "print(f\"Used samples: {test_used} / {len(test_labels)}\")\n",
    "print(f\"Rejected samples: {len(test_labels) - test_used}\")\n",
    "\n",
    "# L\u01b0u k\u1ebft qu\u1ea3 test ngay l\u1eadp t\u1ee9c\n",
    "test_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'eval_time_seconds': eval_time,\n",
    "    'threshold': int(best_threshold),\n",
    "    'accuracy': float(test_acc),\n",
    "    'coverage': float(test_cov),\n",
    "    'used_samples': int(test_used),\n",
    "    'total_samples': len(test_labels),\n",
    "    'rejected_samples': len(test_labels) - test_used,\n",
    "    'confidence_stats': {\n",
    "        'min': float(test_confidences.min()),\n",
    "        'max': float(test_confidences.max()),\n",
    "        'mean': float(test_confidences.mean()),\n",
    "        'std': float(test_confidences.std())\n",
    "    }\n",
    "}\n",
    "with open(f'{LOG_DIR}/test_evaluation.json', 'w') as f:\n",
    "    json.dump(test_data, f, indent=2)\n",
    "print(f\"\\n[OK] Test results saved to {LOG_DIR}/test_evaluation.json\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Visualizations\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold Analysis Plot\n",
    "thresholds = [r[0] for r in threshold_results]\n",
    "accuracies = [r[1] for r in threshold_results]\n",
    "coverages = [r[2] for r in threshold_results]\n",
    "scores = [r[3] for r in threshold_results]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy vs Threshold\n",
    "axes[0].plot(thresholds, accuracies, 'b-o', label='Accuracy')\n",
    "axes[0].axvline(best_threshold, color='r', linestyle='--', label=f'Best threshold={best_threshold}')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy vs Threshold')\n",
    "axes[0].grid(True)\n",
    "axes[0].legend()\n",
    "\n",
    "# Coverage vs Threshold\n",
    "axes[1].plot(thresholds, coverages, 'g-o', label='Coverage')\n",
    "axes[1].axvline(best_threshold, color='r', linestyle='--', label=f'Best threshold={best_threshold}')\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Coverage')\n",
    "axes[1].set_title('Coverage vs Threshold')\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "\n",
    "# Score vs Threshold\n",
    "axes[2].plot(thresholds, scores, 'm-o', label='Score (acc * cov)')\n",
    "axes[2].axvline(best_threshold, color='r', linestyle='--', label=f'Best threshold={best_threshold}')\n",
    "axes[2].set_xlabel('Threshold')\n",
    "axes[2].set_ylabel('Score')\n",
    "axes[2].set_title('Score (Accuracy * Coverage) vs Threshold')\n",
    "axes[2].grid(True)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/lbph_threshold_analysis.png', dpi=150)\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence Distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(test_confidences, bins=50, edgecolor='black')\n",
    "plt.axvline(best_threshold, color='r', linestyle='--', linewidth=2, label=f'Threshold={best_threshold}')\n",
    "plt.xlabel('Confidence (lower is better)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Test Set Confidence Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Predictions \u0111\u1ec3 t\u00ednh confusion matrix\n",
    "test_predictions = []\n",
    "test_true_labels_filtered = []\n",
    "for img, true_label in zip(test_faces, test_labels):\n",
    "    pred, conf = model.predict(img)\n",
    "    if conf < best_threshold:\n",
    "        test_predictions.append(pred)\n",
    "        test_true_labels_filtered.append(true_label)\n",
    "\n",
    "if len(test_predictions) > 0:\n",
    "    cm = confusion_matrix(test_true_labels_filtered, test_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix (Threshold={best_threshold})')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No predictions above threshold', ha='center', va='center')\n",
    "    plt.title('Confusion Matrix (No data)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/lbph_confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Export Results\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CSV v\u00e0 JSON\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    os.system(\"pip install -q pandas\")\n",
    "    import pandas as pd\n",
    "\n",
    "# 1. Export predictions CSV\n",
    "predictions_data = []\n",
    "for img, true_label in zip(test_faces, test_labels):\n",
    "    pred, conf = model.predict(img)\n",
    "    predictions_data.append({\n",
    "        'true_label': int(true_label),\n",
    "        'pred_label': int(pred),\n",
    "        'confidence': float(conf),\n",
    "        'accepted': conf < best_threshold,\n",
    "        'is_correct': pred == true_label if conf < best_threshold else False\n",
    "    })\n",
    "\n",
    "df_predictions = pd.DataFrame(predictions_data)\n",
    "df_predictions.to_csv(f'{OUTPUT_DIR}/lbph_predictions.csv', index=False)\n",
    "print(f\"[OK] Exported predictions CSV: {len(df_predictions)} samples\")\n",
    "\n",
    "# 2. Export threshold results CSV\n",
    "df_thresholds = pd.DataFrame(threshold_results, columns=['threshold', 'accuracy', 'coverage', 'score'])\n",
    "df_thresholds.to_csv(f'{OUTPUT_DIR}/lbph_threshold_results.csv', index=False)\n",
    "print(f\"[OK] Exported threshold results CSV\")\n",
    "\n",
    "# 3. Export evaluation report JSON\n",
    "report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': 'LBPH',\n",
    "    'method': 'threshold-based classification',\n",
    "    'optimal_threshold': int(best_threshold),\n",
    "    'max_images_per_identity': MAX_IMAGES_PER_IDENTITY,\n",
    "    'timing': {\n",
    "        'threshold_search_seconds': threshold_search_time,\n",
    "        'test_eval_seconds': eval_time\n",
    "    },\n",
    "    'metrics': {\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_coverage': float(test_cov),\n",
    "        'test_used_samples': int(test_used),\n",
    "        'test_total_samples': int(len(test_labels))\n",
    "    },\n",
    "    'threshold_results': [\n",
    "        {'threshold': int(t), 'accuracy': float(a), 'coverage': float(c), 'score': float(s)}\n",
    "        for t, a, c, s in threshold_results\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/lbph_evaluation_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"[OK] Exported evaluation report JSON\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"LBPH FINAL REPORT\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Optimal Threshold: {best_threshold}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"Test Coverage: {test_cov:.4f} ({test_cov*100:.2f}%)\")\n",
    "print(f\"Used Samples: {test_used} / {len(test_labels)}\")\n",
    "print(f\"\\nReport saved to: lbph_evaluation_report.json\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip t\u1ea5t c\u1ea3 k\u1ebft qu\u1ea3 \u0111\u1ec3 download\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(OUTPUT_DIR)\n",
    "log_dir = Path(LOG_DIR)\n",
    "zip_path = output_dir / 'lbph_evaluation_results.zip'\n",
    "\n",
    "# Danh s\u00e1ch c\u00e1c file c\u1ea7n zip - bao g\u1ed3m c\u1ea3 intermediate logs\n",
    "files_to_zip = [\n",
    "    # Reports v\u00e0 metrics ch\u00ednh\n",
    "    'lbph_evaluation_report.json',\n",
    "    # CSV files\n",
    "    'lbph_predictions.csv',\n",
    "    'lbph_threshold_results.csv',\n",
    "    # Visualization plots\n",
    "    'lbph_threshold_analysis.png',\n",
    "    'lbph_confusion_matrix.png'\n",
    "]\n",
    "\n",
    "# C\u00e1c file log intermediate\n",
    "log_files = [\n",
    "    'data_info.json',\n",
    "    'threshold_search.json',\n",
    "    'test_evaluation.json'\n",
    "]\n",
    "\n",
    "# T\u1ea1o zip file\n",
    "added_files = []\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Th\u00eam files t\u1eeb output_dir\n",
    "    for file_name in files_to_zip:\n",
    "        file_path = output_dir / file_name\n",
    "        if file_path.exists():\n",
    "            file_size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            zipf.write(file_path, file_name)\n",
    "            print(f\"[OK] Added {file_name} ({file_size_mb:.2f} MB)\")\n",
    "            added_files.append(file_name)\n",
    "        else:\n",
    "            print(f\"[WARNING] {file_name} not found, skipping\")\n",
    "    \n",
    "    # Th\u00eam files t\u1eeb log_dir v\u1edbi prefix logs/\n",
    "    for file_name in log_files:\n",
    "        file_path = log_dir / file_name\n",
    "        if file_path.exists():\n",
    "            file_size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            zipf.write(file_path, f'logs/{file_name}')\n",
    "            print(f\"[OK] Added logs/{file_name} ({file_size_mb:.4f} MB)\")\n",
    "            added_files.append(f'logs/{file_name}')\n",
    "        else:\n",
    "            print(f\"[WARNING] logs/{file_name} not found, skipping\")\n",
    "    \n",
    "    # Th\u00eam confidence distribution data (\u0111\u1ec3 ph\u1ee5c h\u1ed3i n\u1ebfu c\u1ea7n)\n",
    "    conf_data = {'confidences': test_confidences.tolist()}\n",
    "    conf_path = output_dir / 'confidence_distribution.json'\n",
    "    with open(conf_path, 'w') as f:\n",
    "        json.dump(conf_data, f)\n",
    "    zipf.write(conf_path, 'confidence_distribution.json')\n",
    "    added_files.append('confidence_distribution.json')\n",
    "    print(f\"[OK] Added confidence_distribution.json\")\n",
    "\n",
    "# Hi\u1ec3n th\u1ecb th\u00f4ng tin\n",
    "if zip_path.exists():\n",
    "    zip_size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ZIP FILE CREATED: {zip_path.name}\")\n",
    "    print(f\"Size: {zip_size_mb:.2f} MB\")\n",
    "    print(f\"Files included: {len(added_files)}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(\"\\nFiles trong zip:\")\n",
    "    for f in added_files:\n",
    "        print(f\"  - {f}\")\n",
    "    print(\"\\nL\u01b0u \u00fd: Zip file bao g\u1ed3m intermediate logs \u0111\u1ec3 ph\u1ee5c h\u1ed3i k\u1ebft qu\u1ea3 n\u1ebfu session b\u1ecb ng\u1eaft.\")\n",
    "    print(\"\\n\u0110\u1ec3 download:\")\n",
    "    print(\"1. Click v\u00e0o file 'lbph_evaluation_results.zip' trong panel b\u00ean ph\u1ea3i\")\n",
    "    print(\"2. Ho\u1eb7c ch\u1ea1y: !cp lbph_evaluation_results.zip /kaggle/working/\")\n",
    "else:\n",
    "    print(\"[ERROR] Failed to create zip file\")\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}