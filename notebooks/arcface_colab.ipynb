{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "torch cuda: False 12.6\n",
            "torch cuda: False 12.6\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "import torch\n",
        "print(\"torch cuda:\", torch.cuda.is_available(), torch.version.cuda)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è ƒêang ch·∫°y local (kh√¥ng ph·∫£i Colab)\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (ch·ªâ ch·∫°y tr√™n Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IS_COLAB = True\n",
        "    print(\"ƒêang ch·∫°y tr√™n Google Colab\")\n",
        "except:\n",
        "    IS_COLAB = False\n",
        "    print(\"ƒêang ch·∫°y local (kh√¥ng ph·∫£i Colab)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Platform: linux\n",
            "ROOT= /content\n",
            "DRIVE_ROOT= None\n",
            "ROOT exists: True\n",
            "Configs exists: False\n"
          ]
        }
      ],
      "source": [
        "# Set paths (t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh cho local ho·∫∑c Colab)\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if IS_COLAB:\n",
        "    ROOT = \"/content/FaceRecognition\"\n",
        "    DRIVE_ROOT = \"/content/drive/MyDrive/FaceRecognition\"\n",
        "else:\n",
        "    # Local - check if running on Windows path (D:\\ drive)\n",
        "    # Hardcode v√¨ kernel c√≥ th·ªÉ ch·∫°y tr√™n WSL\n",
        "    ROOT = r\"D:\\HCMUTE_project\\DIP\\FaceRecognition\"\n",
        "    # N·∫øu D:\\ kh√¥ng t·ªìn t·∫°i, fallback to current directory\n",
        "    if not os.path.exists(ROOT):\n",
        "        ROOT = os.path.dirname(os.path.abspath(os.getcwd()))\n",
        "        if not os.path.exists(os.path.join(ROOT, 'configs')):\n",
        "            ROOT = os.getcwd()\n",
        "    DRIVE_ROOT = None\n",
        "    \n",
        "if DRIVE_ROOT:\n",
        "    os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
        "    \n",
        "print(\"Platform:\", sys.platform)\n",
        "print(\"ROOT=\", ROOT)\n",
        "print(\"DRIVE_ROOT=\", DRIVE_ROOT)\n",
        "print(f\"ROOT exists: {os.path.exists(ROOT)}\")\n",
        "print(f\"Configs exists: {os.path.exists(os.path.join(ROOT, 'configs'))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ƒêang ch·∫°y local, b·ªè qua clone repo\n",
            "Working directory: /content\n"
          ]
        }
      ],
      "source": [
        "# Clone/Update repository (ch·ªâ tr√™n Colab)\n",
        "if IS_COLAB:\n",
        "    import subprocess\n",
        "    import shutil\n",
        "    from getpass import getpass\n",
        "    \n",
        "    REPO_URL = \"https://github.com/sin0235/FaceRecognition.git\"\n",
        "    \n",
        "    # Ki·ªÉm tra xem ƒë√£ c√≥ git credentials ch∆∞a\n",
        "    if not os.path.exists(ROOT) or not os.path.exists(os.path.join(ROOT, \".git\")):\n",
        "        print(\"=== ƒêƒÇNG NH·∫¨P GITHUB ===\")\n",
        "        print(\"ƒê·ªÉ clone private repo, c·∫ßn GitHub Personal Access Token\")\n",
        "        print(\"T·∫°o token t·∫°i: https://github.com/settings/tokens\")\n",
        "        print(\"Quy·ªÅn c·∫ßn: repo (Full control of private repositories)\\n\")\n",
        "        \n",
        "        github_token = getpass(\"Nh·∫≠p GitHub Token (hidden): \")\n",
        "        \n",
        "        if github_token.strip():\n",
        "            # Configure git credentials\n",
        "            !git config --global credential.helper store\n",
        "            \n",
        "            # Th√™m token v√†o URL\n",
        "            username = \"sin0235\"\n",
        "            auth_url = f\"https://{username}:{github_token}@github.com/sin0235/FaceRecognition.git\"\n",
        "            \n",
        "            if os.path.exists(ROOT):\n",
        "                print(\"X√≥a th∆∞ m·ª•c c≈©...\")\n",
        "                shutil.rmtree(ROOT)\n",
        "            \n",
        "            print(f\"ƒêang clone repository...\")\n",
        "            result = subprocess.run(['git', 'clone', auth_url, ROOT], \n",
        "                                  capture_output=True, text=True)\n",
        "            \n",
        "            if result.returncode == 0 and os.path.exists(ROOT):\n",
        "                print(\"Clone th√†nh c√¥ng!\")\n",
        "                os.chdir(ROOT)\n",
        "                \n",
        "                # Chuy·ªÉn v·ªÅ HTTPS URL kh√¥ng c√≥ token ƒë·ªÉ tr√°nh l·ªô\n",
        "                subprocess.run(['git', 'remote', 'set-url', 'origin', REPO_URL], \n",
        "                             capture_output=True)\n",
        "            else:\n",
        "                print(\"Clone th·∫•t b·∫°i!\")\n",
        "                print(\"Error:\", result.stderr)\n",
        "        else:\n",
        "            print(\"Kh√¥ng c√≥ token, th·ª≠ copy t·ª´ Drive...\")\n",
        "            if DRIVE_ROOT and os.path.exists(DRIVE_ROOT):\n",
        "                print(f\"Copy code t·ª´ {DRIVE_ROOT} sang {ROOT}\")\n",
        "                shutil.copytree(DRIVE_ROOT, ROOT, dirs_exist_ok=True)\n",
        "                if os.path.exists(ROOT):\n",
        "                    print(\"Copy th√†nh c√¥ng!\")\n",
        "                    os.chdir(ROOT)\n",
        "            else:\n",
        "                print(\"Kh√¥ng t√¨m th·∫•y code trong Drive!\")\n",
        "    else:\n",
        "        print(\"Repository ƒë√£ t·ªìn t·∫°i\")\n",
        "        if os.path.exists(os.path.join(ROOT, \".git\")):\n",
        "            print(\"ƒêang pull updates...\")\n",
        "            os.chdir(ROOT)\n",
        "            result = subprocess.run(['git', 'pull'], capture_output=True, text=True)\n",
        "            if result.returncode == 0:\n",
        "                print(\"Pull th√†nh c√¥ng!\")\n",
        "            else:\n",
        "                print(\"Pull warning:\", result.stderr)\n",
        "        else:\n",
        "            os.chdir(ROOT)\n",
        "    \n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"ƒêang ch·∫°y local, b·ªè qua clone repo\")\n",
        "    if os.path.exists(ROOT):\n",
        "        os.chdir(ROOT)\n",
        "    print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è ƒêang ch·∫°y local, b·ªè qua sync t·ª´ Drive\n"
          ]
        }
      ],
      "source": [
        "# Sync d·ªØ li·ªáu t·ª´ Drive (ch·ªâ tr√™n Colab)\n",
        "if IS_COLAB and DRIVE_ROOT:\n",
        "    import shutil\n",
        "    if os.path.exists(os.path.join(DRIVE_ROOT, \"data\")):\n",
        "        print(\"ƒêang sync d·ªØ li·ªáu t·ª´ Drive...\")\n",
        "        shutil.copytree(os.path.join(DRIVE_ROOT, \"data\"), \n",
        "                        os.path.join(ROOT, \"data\"), \n",
        "                        dirs_exist_ok=True)\n",
        "        print(\"Sync ho√†n t·∫•t\")\n",
        "    else:\n",
        "        print(\"Ch∆∞a c√≥ d·ªØ li·ªáu trong Drive, b·ªè qua sync\")\n",
        "else:\n",
        "    print(\"ƒêang ch·∫°y local, b·ªè qua sync t·ª´ Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è ƒêang ch·∫°y local, b·ªè qua c√†i ƒë·∫∑t dependencies\n",
            "ƒê·∫£m b·∫£o b·∫°n ƒë√£ c√†i ƒë·∫∑t: pip install -r requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (ch·ªâ tr√™n Colab)\n",
        "if IS_COLAB:\n",
        "    print(\"C√†i ƒë·∫∑t PyTorch v√† dependencies...\")\n",
        "    \n",
        "    # G·ª° torchaudio n·∫øu c√≥ conflict\n",
        "    %pip uninstall -y torchaudio\n",
        "    \n",
        "    # C√†i PyTorch\n",
        "    %pip install -q torch==2.5.1+cu118 torchvision==0.20.1+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "    \n",
        "    # C√†i c√°c package c∆° b·∫£n (kh√¥ng d√πng requirements.txt ƒë·ªÉ tr√°nh version c≈©)\n",
        "    %pip install -q onnxruntime-gpu insightface\n",
        "    %pip install -q numpy pandas opencv-python-headless Pillow scikit-learn matplotlib tqdm pyyaml\n",
        "    \n",
        "    print(\"Ho√†n t·∫•t c√†i ƒë·∫∑t dependencies!\")\n",
        "else:\n",
        "    print(\"ƒêang ch·∫°y local, b·ªè qua c√†i ƒë·∫∑t dependencies\")\n",
        "    print(\"ƒê·∫£m b·∫£o b·∫°n ƒë√£ c√†i ƒë·∫∑t: pip install -r requirements.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unzip meta_data_celeba.zip t·ª´ Drive v√† l∆∞u v√†o Drive\n",
        "if IS_COLAB and DRIVE_ROOT:\n",
        "    import zipfile\n",
        "    \n",
        "    zip_path = os.path.join(DRIVE_ROOT, \"data\", \"meta_data_celeba.zip\")\n",
        "    extract_to = os.path.join(DRIVE_ROOT, \"data\")\n",
        "    \n",
        "    if os.path.exists(zip_path):\n",
        "        print(f\"ƒêang gi·∫£i n√©n {zip_path}...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        print(f\"Gi·∫£i n√©n ho√†n t·∫•t v√†o: {extract_to}\")\n",
        "        \n",
        "        # Li·ªát k√™ c√°c file ƒë√£ gi·∫£i n√©n\n",
        "        extracted_files = os.listdir(extract_to)\n",
        "        print(f\"\\nC√°c file trong {extract_to}:\")\n",
        "        for f in sorted(extracted_files)[:15]:\n",
        "            path = os.path.join(extract_to, f)\n",
        "            if os.path.isfile(path):\n",
        "                size = os.path.getsize(path) / (1024*1024)\n",
        "                print(f\"  - {f} ({size:.2f} MB)\")\n",
        "            else:\n",
        "                print(f\"  - {f}/ (folder)\")\n",
        "    else:\n",
        "        print(f\"Kh√¥ng t√¨m th·∫•y file: {zip_path}\")\n",
        "else:\n",
        "    print(\"ƒêang ch·∫°y local, b·ªè qua unzip t·ª´ Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch cuda: False\n",
            "  - ‚ö†Ô∏è CUDA kh√¥ng kh·∫£ d·ª•ng, s·∫Ω s·ª≠ d·ª•ng CPU\n",
            "mxnet: ch∆∞a c√†i ƒë·∫∑t (optional)\n",
            "onnxruntime: ch∆∞a c√†i ƒë·∫∑t (optional)\n"
          ]
        }
      ],
      "source": [
        "# Ki·ªÉm tra GPU v√† dependencies\n",
        "import torch\n",
        "print(\"=== GPU INFO ===\")\n",
        "print(f\"torch cuda: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  - CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"  - Device name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  - Device count: {torch.cuda.device_count()}\")\n",
        "else:\n",
        "    print(\"  - CUDA kh√¥ng kh·∫£ d·ª•ng, s·∫Ω s·ª≠ d·ª•ng CPU\")\n",
        "\n",
        "print(\"\\n=== DEPENDENCIES ===\")\n",
        "try:\n",
        "    import onnxruntime as ort\n",
        "    providers = ort.get_available_providers()\n",
        "    print(f\"onnxruntime: OK (providers: {providers})\")\n",
        "except ImportError:\n",
        "    print(\"onnxruntime: ch∆∞a c√†i ƒë·∫∑t\")\n",
        "\n",
        "try:\n",
        "    import insightface\n",
        "    print(f\"insightface: OK (version: {insightface.__version__})\")\n",
        "except ImportError:\n",
        "    print(\"insightface: ch∆∞a c√†i ƒë·∫∑t\")\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "    print(f\"opencv: OK\")\n",
        "except ImportError:\n",
        "    print(\"opencv: ch∆∞a c√†i ƒë·∫∑t\")\n",
        "\n",
        "print(\"\\nS·∫µn s√†ng ƒë·ªÉ training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CHECKPOINT_DIR= /content/models/checkpoints\n",
            "DATA_DIR= /content/data\n"
          ]
        }
      ],
      "source": [
        "# Prepare data/checkpoints paths\n",
        "if IS_COLAB and DRIVE_ROOT:\n",
        "    CHECKPOINT_DIR = os.path.join(DRIVE_ROOT, \"models\", \"checkpoints\")\n",
        "    DATA_DIR = os.path.join(DRIVE_ROOT, \"data\")\n",
        "else:\n",
        "    # Local paths\n",
        "    CHECKPOINT_DIR = os.path.join(ROOT, \"models\", \"checkpoints\")\n",
        "    DATA_DIR = os.path.join(ROOT, \"data\")\n",
        "    \n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "print(\"CHECKPOINT_DIR=\", CHECKPOINT_DIR)\n",
        "print(\"DATA_DIR=\", DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== N·ªôi dung ROOT ===\n",
            "ROOT: /content\n",
            "Found 6 items:\n",
            "  - .config\n",
            "  - FaceRecognition\n",
            "  - data\n",
            "  - drive\n",
            "  - models\n",
            "  - sample_data\n",
            "\n",
            "=== File Paths ===\n",
            "CONFIG_PATH: /content/configs/arcface_config.yaml\n",
            "Config exists: False\n",
            "PRETRAINED: /content/models/checkpoints/ms1mv2_resnet50.pth\n",
            "Pretrained exists: False\n"
          ]
        }
      ],
      "source": [
        "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n v√† ki·ªÉm tra files\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# List ROOT directory\n",
        "print(\"=== N·ªôi dung ROOT ===\")\n",
        "print(f\"ROOT: {ROOT}\")\n",
        "if os.path.exists(ROOT):\n",
        "    items = sorted(os.listdir(ROOT))\n",
        "    print(f\"Found {len(items)} items:\")\n",
        "    for item in items[:25]:\n",
        "        print(f\"  - {item}\")\n",
        "else:\n",
        "    print(\"ROOT kh√¥ng t·ªìn t·∫°i!\")\n",
        "\n",
        "# Config paths\n",
        "CONFIG_PATH = os.path.join(ROOT, \"configs\", \"arcface_config.yaml\")\n",
        "PRETRAINED_BACKBONE = os.path.join(CHECKPOINT_DIR, \"ms1mv2_resnet50.pth\")\n",
        "\n",
        "print(f\"\\n=== File Paths ===\")\n",
        "print(f\"CONFIG_PATH: {CONFIG_PATH}\")\n",
        "print(f\"Config exists: {os.path.exists(CONFIG_PATH)}\")\n",
        "print(f\"PRETRAINED: {PRETRAINED_BACKBONE}\")\n",
        "print(f\"Pretrained exists: {os.path.exists(PRETRAINED_BACKBONE)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== KI·ªÇM TRA D·ªÆ LI·ªÜU ===\n",
            "\n",
            "‚ùå Kh√¥ng t√¨m th·∫•y: /content/data/processed/train_metadata.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "‚ùå Kh√¥ng t√¨m th·∫•y: /content/data/processed/val_metadata.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "‚ùå Kh√¥ng t√¨m th·∫•y: /content/data/processed/train\n",
            "‚ùå Kh√¥ng t√¨m th·∫•y: /content/data/processed/val\n"
          ]
        }
      ],
      "source": [
        "# Ki·ªÉm tra d·ªØ li·ªáu\n",
        "import pandas as pd\n",
        "\n",
        "data_processed_dir = os.path.join(ROOT, \"data\", \"processed\")\n",
        "train_csv_path = os.path.join(data_processed_dir, \"train_metadata.csv\")\n",
        "val_csv_path = os.path.join(data_processed_dir, \"val_metadata.csv\")\n",
        "\n",
        "print(\"=== KI·ªÇM TRA D·ªÆ LI·ªÜU ===\\n\")\n",
        "\n",
        "# Ki·ªÉm tra train metadata\n",
        "if os.path.exists(train_csv_path):\n",
        "    train_df = pd.read_csv(train_csv_path)\n",
        "    print(f\"Train metadata: {len(train_df)} ·∫£nh\")\n",
        "    print(f\"  - S·ªë identities: {train_df['identity_name'].nunique()}\")\n",
        "    print(f\"  - Columns: {list(train_df.columns)}\")\n",
        "    print(f\"\\nSample:\")\n",
        "    print(train_df.head(3))\n",
        "else:\n",
        "    print(f\"Kh√¥ng t√¨m th·∫•y: {train_csv_path}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Ki·ªÉm tra val metadata\n",
        "if os.path.exists(val_csv_path):\n",
        "    val_df = pd.read_csv(val_csv_path)\n",
        "    print(f\"Val metadata: {len(val_df)} ·∫£nh\")\n",
        "    print(f\"  - S·ªë identities: {val_df['identity_name'].nunique()}\")\n",
        "else:\n",
        "    print(f\"Kh√¥ng t√¨m th·∫•y: {val_csv_path}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Ki·ªÉm tra th∆∞ m·ª•c ·∫£nh\n",
        "train_dir = os.path.join(data_processed_dir, \"train\")\n",
        "val_dir = os.path.join(data_processed_dir, \"val\")\n",
        "\n",
        "if os.path.exists(train_dir):\n",
        "    train_folders = [f for f in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, f))]\n",
        "    print(f\"Train images dir: {len(train_folders)} folders\")\n",
        "else:\n",
        "    print(f\"Kh√¥ng t√¨m th·∫•y: {train_dir}\")\n",
        "\n",
        "if os.path.exists(val_dir):\n",
        "    val_folders = [f for f in os.listdir(val_dir) if os.path.isdir(os.path.join(val_dir, f))]\n",
        "    print(f\"Val images dir: {len(val_folders)} folders\")\n",
        "else:\n",
        "    print(f\"Kh√¥ng t√¨m th·∫•y: {val_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Thi·∫øu training script: /content/models/arcface/train_arcface.py\n"
          ]
        }
      ],
      "source": [
        "# Ki·ªÉm tra v√† ch·∫°y training\n",
        "TRAIN_SCRIPT = os.path.join(ROOT, \"models\", \"arcface\", \"train_arcface.py\")\n",
        "\n",
        "if not os.path.exists(TRAIN_SCRIPT):\n",
        "    print(f\"Thi·∫øu training script: {TRAIN_SCRIPT}\")\n",
        "else:\n",
        "    print(f\"Training script t·ªìn t·∫°i: {TRAIN_SCRIPT}\")\n",
        "    \n",
        "    # Ki·ªÉm tra d·ªØ li·ªáu\n",
        "    train_csv = os.path.join(ROOT, \"data\", \"processed\", \"train_metadata.csv\")\n",
        "    val_csv = os.path.join(ROOT, \"data\", \"processed\", \"val_metadata.csv\")\n",
        "    \n",
        "    if not os.path.exists(train_csv):\n",
        "        print(f\"Ch∆∞a c√≥ d·ªØ li·ªáu training: {train_csv}\")\n",
        "        print(\"ƒê·ªÉ ti·∫øp t·ª•c, c·∫ßn chu·∫©n b·ªã:\")\n",
        "        print(\"  1. Upload d·ªØ li·ªáu ƒë√£ preprocess l√™n Drive\")\n",
        "        print(f\"  2. ƒê·∫£m b·∫£o c√≥ file: {os.path.join(DATA_DIR, 'processed/train_metadata.csv')}\")\n",
        "        print(\"  3. Ch·∫°y l·∫°i cell 5 ƒë·ªÉ sync d·ªØ li·ªáu\")\n",
        "    elif not os.path.exists(val_csv):\n",
        "        print(f\"Ch∆∞a c√≥ d·ªØ li·ªáu validation: {val_csv}\")\n",
        "        print(\"Vui l√≤ng chu·∫©n b·ªã d·ªØ li·ªáu tr∆∞·ªõc khi training!\")\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"B·∫ÆT ƒê·∫¶U TRAINING\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        # Ch·∫°y training v·ªõi ƒë·∫ßy ƒë·ªß arguments\n",
        "        !python $TRAIN_SCRIPT \\\n",
        "            --config $CONFIG_PATH \\\n",
        "            --pretrained_backbone $PRETRAINED_BACKBONE \\\n",
        "            --data_dir $ROOT/data \\\n",
        "            --checkpoint_dir $CHECKPOINT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'models.arcface'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3798351649.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marcface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marcface_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing ArcFace Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models.arcface'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Test model sau khi training\n",
        "import sys\n",
        "sys.path.append(ROOT)\n",
        "\n",
        "from models.arcface.arcface_model import test_model\n",
        "\n",
        "print(\"Testing ArcFace Model...\")\n",
        "test_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best checkpoint v√† extract embeddings\n",
        "import torch\n",
        "from models.arcface.arcface_model import ArcFaceModel\n",
        "\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, \"arcface_best.pth\")\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
        "    \n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    num_classes = len(checkpoint['config']['data'].get('num_classes', 100))\n",
        "    \n",
        "    model = ArcFaceModel(num_classes=num_classes, embedding_size=512)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"Loaded best model - Epoch {checkpoint['epoch']}\")\n",
        "    print(f\"Validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
        "    \n",
        "    # Test inference\n",
        "    dummy_input = torch.randn(1, 3, 112, 112)\n",
        "    with torch.no_grad():\n",
        "        embedding = model.extract_features(dummy_input)\n",
        "    \n",
        "    print(f\"Embedding shape: {embedding.shape}\")\n",
        "    print(\"Model s·∫µn s√†ng cho inference!\")\n",
        "else:\n",
        "    print(f\"Kh√¥ng t√¨m th·∫•y checkpoint: {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Backup/Sync k·∫øt qu·∫£ training l√™n Drive\n",
        "if IS_COLAB and DRIVE_ROOT:\n",
        "    import shutil\n",
        "    from pathlib import Path\n",
        "    \n",
        "    print(\"=== BACKUP K·∫æT QU·∫¢ L√äN DRIVE ===\\n\")\n",
        "    \n",
        "    # 1. Backup checkpoints\n",
        "    local_checkpoint_dir = os.path.join(ROOT, \"models\", \"checkpoints\")\n",
        "    drive_checkpoint_dir = os.path.join(DRIVE_ROOT, \"models\", \"checkpoints\")\n",
        "    \n",
        "    if os.path.exists(local_checkpoint_dir):\n",
        "        os.makedirs(drive_checkpoint_dir, exist_ok=True)\n",
        "        print(f\"ƒêang backup checkpoints...\")\n",
        "        \n",
        "        for file in os.listdir(local_checkpoint_dir):\n",
        "            if file.endswith(('.pth', '.pt')):\n",
        "                src = os.path.join(local_checkpoint_dir, file)\n",
        "                dst = os.path.join(drive_checkpoint_dir, file)\n",
        "                shutil.copy2(src, dst)\n",
        "                size = os.path.getsize(src) / (1024*1024)\n",
        "                print(f\"  {file} ({size:.2f} MB)\")\n",
        "    \n",
        "    # 2. Backup logs\n",
        "    local_log_dir = os.path.join(ROOT, \"logs\")\n",
        "    drive_log_dir = os.path.join(DRIVE_ROOT, \"logs\")\n",
        "    \n",
        "    if os.path.exists(local_log_dir):\n",
        "        print(f\"\\nƒêang backup logs...\")\n",
        "        shutil.copytree(local_log_dir, drive_log_dir, dirs_exist_ok=True)\n",
        "        print(f\"  Logs ƒë√£ ƒë∆∞·ª£c backup\")\n",
        "    \n",
        "    # 3. Backup embeddings n·∫øu c√≥\n",
        "    local_embeddings = os.path.join(ROOT, \"data\", \"embeddings\")\n",
        "    drive_embeddings = os.path.join(DRIVE_ROOT, \"data\", \"embeddings\")\n",
        "    \n",
        "    if os.path.exists(local_embeddings):\n",
        "        print(f\"\\nƒêang backup embeddings...\")\n",
        "        shutil.copytree(local_embeddings, drive_embeddings, dirs_exist_ok=True)\n",
        "        print(f\"  Embeddings ƒë√£ ƒë∆∞·ª£c backup\")\n",
        "    \n",
        "    # 4. Backup processed data metadata\n",
        "    for csv_file in ['train_metadata.csv', 'val_metadata.csv']:\n",
        "        local_csv = os.path.join(ROOT, \"data\", \"processed\", csv_file)\n",
        "        drive_csv = os.path.join(DRIVE_ROOT, \"data\", \"processed\", csv_file)\n",
        "        \n",
        "        if os.path.exists(local_csv):\n",
        "            os.makedirs(os.path.dirname(drive_csv), exist_ok=True)\n",
        "            shutil.copy2(local_csv, drive_csv)\n",
        "            print(f\"  {csv_file}\")\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"BACKUP HO√ÄN T·∫§T! D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o Drive\")\n",
        "    print(f\"{'='*50}\")\n",
        "else:\n",
        "    print(\"ƒêang ch·∫°y local, b·ªè qua backup l√™n Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unzip meta_data_celeba.zip t·ª´ Drive v√†o cache c·ªßa phi√™n ch·∫°y\n",
        "if IS_COLAB and DRIVE_ROOT:\n",
        "    import subprocess\n",
        "    \n",
        "    zip_path = os.path.join(DRIVE_ROOT, \"data\", \"meta_data_celeba.zip\")\n",
        "    # Gi·∫£i n√©n v√†o th∆∞ m·ª•c t·∫°m c·ªßa phi√™n ch·∫°y (cache)\n",
        "    cache_dir = \"/tmp/celeba_metadata\"\n",
        "    \n",
        "    if os.path.exists(zip_path):\n",
        "        print(f\"ƒêang gi·∫£i n√©n {zip_path} v√†o cache...\")\n",
        "        print(f\"ƒê√≠ch: {cache_dir}\\n\")\n",
        "        \n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        \n",
        "        result = subprocess.run(\n",
        "            ['unzip', '-q', '-o', zip_path, '-d', cache_dir],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(f\"Gi·∫£i n√©n ho√†n t·∫•t v√†o: {cache_dir}\")\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(\"L∆ØU √ù: D·ªØ li·ªáu ƒëang ·ªü cache c·ªßa phi√™n ch·∫°y\")\n",
        "            print(\"B·∫°n c·∫ßn CHUY·ªÇN TH·ª¶ C√îNG v√†o Drive n·∫øu mu·ªën l∆∞u l√¢u d√†i\")\n",
        "            print(f\"{'='*60}\\n\")\n",
        "            \n",
        "            # Li·ªát k√™ c√°c file ƒë√£ gi·∫£i n√©n\n",
        "            extracted_files = os.listdir(cache_dir)\n",
        "            print(f\"C√°c file trong {cache_dir}:\")\n",
        "            for f in sorted(extracted_files)[:20]:\n",
        "                path = os.path.join(cache_dir, f)\n",
        "                if os.path.isfile(path):\n",
        "                    size = os.path.getsize(path) / (1024*1024)\n",
        "                    print(f\"  - {f} ({size:.2f} MB)\")\n",
        "                else:\n",
        "                    print(f\"  - {f}/ (th∆∞ m·ª•c)\")\n",
        "            \n",
        "            print(f\"\\nüìù ƒê·ªÉ chuy·ªÉn v√†o Drive, ch·∫°y l·ªánh:\")\n",
        "            print(f\"!cp -r {cache_dir}/* {os.path.join(DRIVE_ROOT, 'data')}/\")\n",
        "        else:\n",
        "            print(f\"L·ªói gi·∫£i n√©n: {result.stderr}\")\n",
        "    else:\n",
        "        print(f\"Kh√¥ng t√¨m th·∫•y file: {zip_path}\")\n",
        "        print(f\"Ki·ªÉm tra xem file ƒë√£ ƒë∆∞·ª£c upload v√†o Drive ch∆∞a\")\n",
        "else:\n",
        "    print(\"ƒêang ch·∫°y local, b·ªè qua unzip t·ª´ Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimizing tool selection...T√¥i s·∫Ω th√™m m·ªôt cell m·ªõi ƒë·ªÉ unzip file `meta_data_celeba.zip` t·ª´ Google Drive. \n",
        "\n",
        "Edit [](file:///d%3A/HCMUTE_project/DIP/FaceRecognition/notebooks/arcface_colab.ipynb)\n",
        "\n",
        "ƒê√£ th√™m cell unzip file `meta_data_celeba.zip` v√†o notebook. Cell n√†y s·∫Ω:\n",
        "\n",
        "- Ki·ªÉm tra xem c√≥ ƒëang ch·∫°y tr√™n Colab kh√¥ng\n",
        "- T√¨m file zip t·∫°i `/content/drive/MyDrive/FaceRecognition/data/meta_data_celeba.zip`\n",
        "- Gi·∫£i n√©n v√†o th∆∞ m·ª•c data c·ªßa project\n",
        "- Hi·ªÉn th·ªã danh s√°ch c√°c file ƒë√£ gi·∫£i n√©n\n",
        "\n",
        "Cell m·ªõi ƒë∆∞·ª£c ƒë·∫∑t ngay sau cell c√†i ƒë·∫∑t dependencies ƒë·ªÉ ƒë·∫£m b·∫£o metadata ƒë∆∞·ª£c gi·∫£i n√©n tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu training.\n",
        "\n",
        "Made changes."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
