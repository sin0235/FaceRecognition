{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBPH Evaluation - Kaggle\n",
    "\n",
    "Evaluation notebook cho LBPH model sử dụng threshold-based classification.\n",
    "\n",
    "## Approach:\n",
    "- Load LBPH model từ checkpoint (XML file)\n",
    "- Load validation và test datasets\n",
    "- Tìm threshold tối ưu trên validation set\n",
    "- Evaluate trên test set với threshold đã chọn\n",
    "- Metrics: Accuracy, Coverage, Threshold analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, json\n",
    "import shutil, glob\n",
    "from datetime import datetime\n",
    "# numpy và matplotlib sẽ được import sau khi fix compatibility\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "ROOT = \"/kaggle/working/FaceRecognition\"\n",
    "CHECKPOINT_DIR = \"/kaggle/working/checkpoints/lbph\"\n",
    "KAGGLE_DATASET_NAME = \"celeba-aligned-balanced\"\n",
    "DATA_DIR = f\"/kaggle/input/{KAGGLE_DATASET_NAME}\"\n",
    "CHECKPOINT_DATASET_NAME = \"lbph-checkpoints\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy checkpoint từ Kaggle input dataset (nếu có)\n",
    "checkpoint_input = f\"/kaggle/input/{CHECKPOINT_DATASET_NAME}\"\n",
    "checkpoint_found = False\n",
    "\n",
    "if os.path.exists(checkpoint_input):\n",
    "    for xml_file in glob.glob(os.path.join(checkpoint_input, \"**/*.xml\"), recursive=True):\n",
    "        dest = os.path.join(CHECKPOINT_DIR, os.path.basename(xml_file))\n",
    "        if not os.path.exists(dest) or os.path.getsize(dest) != os.path.getsize(xml_file):\n",
    "            print(f\"Copying {os.path.basename(xml_file)} from Kaggle input...\")\n",
    "            shutil.copy(xml_file, dest)\n",
    "            if os.path.getsize(dest) == os.path.getsize(xml_file):\n",
    "                print(f\"  [OK] Copied successfully\")\n",
    "                checkpoint_found = True\n",
    "            else:\n",
    "                print(f\"  [WARNING] Size mismatch after copy!\")\n",
    "        else:\n",
    "            checkpoint_found = True\n",
    "    if checkpoint_found:\n",
    "        print(f\"Checkpoints from Kaggle input: {os.listdir(CHECKPOINT_DIR)}\")\n",
    "else:\n",
    "    print(f\"[INFO] Không tìm thấy checkpoint trong Kaggle input: {checkpoint_input}\")\n",
    "    print(f\"      Sẽ thử copy từ repo sau khi clone...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cau hinh GitHub token\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "    print(\"[OK] Da lay GITHUB_TOKEN\")\n",
    "except Exception as e:\n",
    "    GITHUB_TOKEN = None\n",
    "    print(\"[INFO] Su dung public URL\")\n",
    "\n",
    "if GITHUB_TOKEN:\n",
    "    REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/sin0235/FaceRecognition.git\"\n",
    "else:\n",
    "    REPO_URL = \"https://github.com/sin0235/FaceRecognition.git\"\n",
    "\n",
    "# Clone repository\n",
    "if os.path.exists(ROOT):\n",
    "    print(\"Repository da ton tai, dang pull updates...\")\n",
    "    %cd {ROOT}\n",
    "    if GITHUB_TOKEN:\n",
    "        !git remote set-url origin {REPO_URL}\n",
    "    !git pull --no-rebase origin fix/lbph-module\n",
    "else:\n",
    "    print(f\"Dang clone repository...\")\n",
    "    !git clone {REPO_URL} {ROOT}\n",
    "    %cd {ROOT}\n",
    "\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
    "!ls -la\n",
    "\n",
    "# Thêm ROOT vào sys.path để import modules\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "    print(f\"\\n[OK] Added {ROOT} to sys.path\")\n",
    "\n",
    "# Copy checkpoint từ repo nếu chưa có từ Kaggle input\n",
    "model_path_check = os.path.join(CHECKPOINT_DIR, \"lbph_model.xml\")\n",
    "if not os.path.exists(model_path_check):\n",
    "    repo_checkpoint_path = os.path.join(ROOT, \"models\", \"checkpoints\", \"LBHP\", \"lbph_model.xml\")\n",
    "    if os.path.exists(repo_checkpoint_path):\n",
    "        print(f\"\\nCopying checkpoint from repo: {repo_checkpoint_path}\")\n",
    "        shutil.copy(repo_checkpoint_path, model_path_check)\n",
    "        if os.path.exists(model_path_check):\n",
    "            file_size = os.path.getsize(model_path_check)\n",
    "            print(f\"  [OK] Copied successfully ({file_size / 1024 / 1024:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"  [ERROR] Failed to copy checkpoint\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Không tìm thấy checkpoint trong repo: {repo_checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"\\n[OK] Checkpoint đã có sẵn: {model_path_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt dependencies với thứ tự đúng để tránh xung đột\n",
    "# Thứ tự: numpy -> scipy -> scikit-learn (phụ thuộc numpy/scipy) -> matplotlib/seaborn\n",
    "!pip install -q --upgrade numpy\n",
    "!pip install -q --upgrade scipy\n",
    "!pip install -q --upgrade --force-reinstall scikit-learn\n",
    "!pip install -q opencv-python-headless opencv-contrib-python-headless Pillow tqdm pyyaml matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorBoard warnings (không ảnh hưởng đến code chính)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import các thư viện cần thiết\n",
    "import sys\n",
    "\n",
    "# Kiểm tra và import numpy, matplotlib, seaborn, sklearn\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "    print(\"[OK] All imports successful\")\n",
    "except (ImportError, AttributeError, TypeError, ValueError) as e:\n",
    "    # Fix compatibility issues với numpy/scipy/matplotlib/sklearn\n",
    "    print(f\"Đang fix compatibility: {e}\")\n",
    "    print(\"Reinstalling numpy, scipy, scikit-learn, matplotlib, seaborn...\")\n",
    "    !pip install -q --upgrade --force-reinstall numpy scipy scikit-learn matplotlib seaborn\n",
    "    \n",
    "    # Xóa TẤT CẢ modules liên quan để import lại từ đầu\n",
    "    modules_to_remove = [k for k in list(sys.modules.keys()) \n",
    "                        if any(x in k for x in ['numpy', 'matplotlib', 'seaborn', 'scipy', 'sklearn', 'pandas'])]\n",
    "    for mod in modules_to_remove:\n",
    "        try:\n",
    "            del sys.modules[mod]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Import lại từ đầu\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "    print(\"[OK] All imports successful after fix\")\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(CHECKPOINT_DIR, \"lbph_model.xml\")\n",
    "\n",
    "# Validate checkpoint file trước khi load\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Model không tồn tại: {model_path}\\n\"\n",
    "        f\"Vui lòng kiểm tra:\"\n",
    "        f\"  1. Dataset checkpoint đã được add vào Kaggle input chưa?\"\n",
    "        f\"  2. Tên dataset có đúng '{CHECKPOINT_DATASET_NAME}' không?\"\n",
    "        f\"  3. File lbph_model.xml có trong dataset không?\"\n",
    "    )\n",
    "\n",
    "file_size = os.path.getsize(model_path)\n",
    "print(f\"Model file: {model_path}\")\n",
    "print(f\"File size: {file_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "if file_size < 1024:\n",
    "    raise ValueError(f\"Model file quá nhỏ ({file_size} bytes), có thể bị hỏng\")\n",
    "\n",
    "# Load LBPH model\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "model.read(model_path)\n",
    "\n",
    "print(f\"\\n[OK] LBPH model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lbphmodel.dataset_lbph import load_data_no_haar\n",
    "\n",
    "# Tìm data dirs\n",
    "train_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"train\")\n",
    "val_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"val\")\n",
    "test_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"test\")\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    train_dir = os.path.join(DATA_DIR, \"train\")\n",
    "    val_dir = os.path.join(DATA_DIR, \"val\")\n",
    "    test_dir = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "print(f\"Train dir: {train_dir}\")\n",
    "print(f\"Val dir: {val_dir}\")\n",
    "print(f\"Test dir: {test_dir}\")\n",
    "\n",
    "# Load validation và test data\n",
    "val_faces, val_labels = load_data_no_haar(val_dir)\n",
    "test_faces, test_labels = load_data_no_haar(test_dir)\n",
    "\n",
    "print(f\"\\nVal samples: {len(val_faces)}\")\n",
    "print(f\"Test samples: {len(test_faces)}\")\n",
    "print(f\"Val labels range: {val_labels.min()} - {val_labels.max()}\")\n",
    "print(f\"Test labels range: {test_labels.min()} - {test_labels.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Find Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lbphmodel.threshold_lbph import find_optimal_threshold\n",
    "\n",
    "# Tìm threshold tối ưu trên validation set\n",
    "print(\"Finding optimal threshold on validation set...\")\n",
    "best_threshold, best_score, threshold_results = find_optimal_threshold(\n",
    "    model, val_faces, val_labels, min_coverage=0.3\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"OPTIMAL THRESHOLD\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Best threshold: {best_threshold}\")\n",
    "print(f\"Best score (acc * coverage): {best_score:.4f}\")\n",
    "print(f\"\\nTop 5 thresholds:\")\n",
    "for thr, acc, cov, score in sorted(threshold_results, key=lambda x: x[3], reverse=True)[:5]:\n",
    "    print(f\"  Threshold={thr:3d}: Accuracy={acc:.3f}, Coverage={cov:.3f}, Score={score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lbphmodel.evaluate_lbph import evaluate_lbph\n",
    "\n",
    "# Evaluate trên test set với threshold đã chọn\n",
    "test_acc, test_cov, test_used, test_confidences = evaluate_lbph(\n",
    "    model, test_faces, test_labels, best_threshold\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TEST SET EVALUATION\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Threshold: {best_threshold}\")\n",
    "print(f\"Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"Coverage: {test_cov:.4f} ({test_cov*100:.2f}%)\")\n",
    "print(f\"Used samples: {test_used} / {len(test_labels)}\")\n",
    "print(f\"Rejected samples: {len(test_labels) - test_used}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold Analysis Plot\n",
    "thresholds = [r[0] for r in threshold_results]\n",
    "accuracies = [r[1] for r in threshold_results]\n",
    "coverages = [r[2] for r in threshold_results]\n",
    "scores = [r[3] for r in threshold_results]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy vs Threshold\n",
    "axes[0].plot(thresholds, accuracies, 'b-o', label='Accuracy')\n",
    "axes[0].axvline(best_threshold, color='r', linestyle='--', label=f'Best threshold={best_threshold}')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy vs Threshold')\n",
    "axes[0].grid(True)\n",
    "axes[0].legend()\n",
    "\n",
    "# Coverage vs Threshold\n",
    "axes[1].plot(thresholds, coverages, 'g-o', label='Coverage')\n",
    "axes[1].axvline(best_threshold, color='r', linestyle='--', label=f'Best threshold={best_threshold}')\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Coverage')\n",
    "axes[1].set_title('Coverage vs Threshold')\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "\n",
    "# Score vs Threshold\n",
    "axes[2].plot(thresholds, scores, 'm-o', label='Score (acc * cov)')\n",
    "axes[2].axvline(best_threshold, color='r', linestyle='--', label=f'Best threshold={best_threshold}')\n",
    "axes[2].set_xlabel('Threshold')\n",
    "axes[2].set_ylabel('Score')\n",
    "axes[2].set_title('Score (Accuracy * Coverage) vs Threshold')\n",
    "axes[2].grid(True)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/lbph_threshold_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence Distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(test_confidences, bins=50, edgecolor='black')\n",
    "plt.axvline(best_threshold, color='r', linestyle='--', linewidth=2, label=f'Threshold={best_threshold}')\n",
    "plt.xlabel('Confidence (lower is better)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Test Set Confidence Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Predictions để tính confusion matrix\n",
    "test_predictions = []\n",
    "test_true_labels_filtered = []\n",
    "for img, true_label in zip(test_faces, test_labels):\n",
    "    pred, conf = model.predict(img)\n",
    "    if conf < best_threshold:\n",
    "        test_predictions.append(pred)\n",
    "        test_true_labels_filtered.append(true_label)\n",
    "\n",
    "if len(test_predictions) > 0:\n",
    "    cm = confusion_matrix(test_true_labels_filtered, test_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix (Threshold={best_threshold})')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No predictions above threshold', ha='center', va='center')\n",
    "    plt.title('Confusion Matrix (No data)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/lbph_confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CSV và JSON\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    !pip install -q pandas\n",
    "    import pandas as pd\n",
    "\n",
    "# 1. Export predictions CSV\n",
    "predictions_data = []\n",
    "for img, true_label in zip(test_faces, test_labels):\n",
    "    pred, conf = model.predict(img)\n",
    "    predictions_data.append({\n",
    "        'true_label': int(true_label),\n",
    "        'pred_label': int(pred),\n",
    "        'confidence': float(conf),\n",
    "        'accepted': conf < best_threshold,\n",
    "        'is_correct': pred == true_label if conf < best_threshold else False\n",
    "    })\n",
    "\n",
    "df_predictions = pd.DataFrame(predictions_data)\n",
    "df_predictions.to_csv('/kaggle/working/lbph_predictions.csv', index=False)\n",
    "print(f\"[OK] Exported predictions CSV: {len(df_predictions)} samples\")\n",
    "\n",
    "# 2. Export threshold results CSV\n",
    "df_thresholds = pd.DataFrame(threshold_results, columns=['threshold', 'accuracy', 'coverage', 'score'])\n",
    "df_thresholds.to_csv('/kaggle/working/lbph_threshold_results.csv', index=False)\n",
    "print(f\"[OK] Exported threshold results CSV\")\n",
    "\n",
    "# 3. Export evaluation report JSON\n",
    "report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': 'LBPH',\n",
    "    'method': 'threshold-based classification',\n",
    "    'optimal_threshold': int(best_threshold),\n",
    "    'metrics': {\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_coverage': float(test_cov),\n",
    "        'test_used_samples': int(test_used),\n",
    "        'test_total_samples': int(len(test_labels))\n",
    "    },\n",
    "    'threshold_results': [\n",
    "        {'threshold': int(t), 'accuracy': float(a), 'coverage': float(c), 'score': float(s)}\n",
    "        for t, a, c, s in threshold_results\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/lbph_evaluation_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"[OK] Exported evaluation report JSON\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"LBPH FINAL REPORT\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Optimal Threshold: {best_threshold}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"Test Coverage: {test_cov:.4f} ({test_cov*100:.2f}%)\")\n",
    "print(f\"Used Samples: {test_used} / {len(test_labels)}\")\n",
    "print(f\"\\nReport saved to: lbph_evaluation_report.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip tất cả kết quả để download\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path('/kaggle/working')\n",
    "zip_path = output_dir / 'lbph_evaluation_results.zip'\n",
    "\n",
    "# Danh sách các file cần zip\n",
    "files_to_zip = [\n",
    "    # Reports và metrics\n",
    "    'lbph_evaluation_report.json',\n",
    "    # CSV files\n",
    "    'lbph_predictions.csv',\n",
    "    'lbph_threshold_results.csv',\n",
    "    # Visualization plots\n",
    "    'lbph_threshold_analysis.png',\n",
    "    'lbph_confusion_matrix.png'\n",
    "]\n",
    "\n",
    "# Tạo zip file\n",
    "added_files = []\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file_name in files_to_zip:\n",
    "        file_path = output_dir / file_name\n",
    "        if file_path.exists():\n",
    "            file_size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            zipf.write(file_path, file_name)\n",
    "            print(f\"[OK] Added {file_name} ({file_size_mb:.2f} MB)\")\n",
    "            added_files.append(file_name)\n",
    "        else:\n",
    "            print(f\"[WARNING] {file_name} not found, skipping\")\n",
    "\n",
    "# Hiển thị thông tin\n",
    "if zip_path.exists():\n",
    "    zip_size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ZIP FILE CREATED: {zip_path.name}\")\n",
    "    print(f\"Size: {zip_size_mb:.2f} MB\")\n",
    "    print(f\"Files included: {len(added_files)}/{len(files_to_zip)}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(\"\\nFiles trong zip:\")\n",
    "    for f in added_files:\n",
    "        print(f\"  - {f}\")\n",
    "    print(\"\\nĐể download:\")\n",
    "    print(\"1. Click vào file 'lbph_evaluation_results.zip' trong panel bên phải\")\n",
    "    print(\"2. Hoặc chạy: !cp lbph_evaluation_results.zip /kaggle/working/\")\n",
    "else:\n",
    "    print(\"[ERROR] Failed to create zip file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
