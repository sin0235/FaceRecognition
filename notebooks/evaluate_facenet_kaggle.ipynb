{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FaceNet Evaluation - Kaggle\n",
        "\n",
        "Evaluation notebook for FaceNet model using embedding-based verification.\n",
        "\n",
        "## Approach:\n",
        "- Extract embeddings for train set (prototypes)\n",
        "- Extract embeddings for eval set\n",
        "- Classification using cosine similarity\n",
        "- Metrics: Top-1/Top-5 accuracy, ROC-AUC, threshold analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, time, json\n",
        "import shutil, glob\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "ROOT = \"/kaggle/working/FaceRecognition\"\n",
        "CHECKPOINT_DIR = \"/kaggle/working/checkpoints/facenet\"\n",
        "KAGGLE_DATASET_NAME = \"celeba-aligned-balanced\"\n",
        "DATA_DIR = f\"/kaggle/input/{KAGGLE_DATASET_NAME}\"\n",
        "CHECKPOINT_DATASET_NAME = \"facenet-checkpoints\"\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy checkpoint\n",
        "checkpoint_input = f\"/kaggle/input/{CHECKPOINT_DATASET_NAME}\"\n",
        "if os.path.exists(checkpoint_input):\n",
        "    for pth in glob.glob(os.path.join(checkpoint_input, \"**/*.pth\"), recursive=True):\n",
        "        dest = os.path.join(CHECKPOINT_DIR, os.path.basename(pth))\n",
        "        if not os.path.exists(dest): shutil.copy(pth, dest)\n",
        "    print(f\"Checkpoints: {os.listdir(CHECKPOINT_DIR)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo\n",
        "REPO_URL = \"https://github.com/sin0235/FaceRecognition.git\"\n",
        "if os.path.exists(ROOT):\n",
        "    %cd {ROOT}\n",
        "    !git pull\n",
        "else:\n",
        "    !git clone {REPO_URL} {ROOT}\n",
        "    %cd {ROOT}\n",
        "if ROOT not in sys.path: sys.path.insert(0, ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q opencv-python-headless Pillow scikit-learn tqdm pyyaml matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Load FaceNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.facenet.facenet_model import FaceNetModel\n",
        "\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, \"facenet_best.pth\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "\n",
        "model = FaceNetModel(embedding_size=128, pretrained=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device).eval()\n",
        "\n",
        "print(f\"Model loaded: embedding_size={128}\")\n",
        "print(f\"Training epochs: {checkpoint.get('epoch', 0) + 1}\")\n",
        "print(f\"Best val acc (training): {checkpoint.get('best_val_acc', 0):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Build Reference Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.facenet.facenet_dataloader import get_val_transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Find data dirs\n",
        "train_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"train\")\n",
        "val_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"val\")\n",
        "test_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"test\")\n",
        "\n",
        "if not os.path.exists(train_dir):\n",
        "    train_dir = os.path.join(DATA_DIR, \"train\")\n",
        "    val_dir = os.path.join(DATA_DIR, \"val\")\n",
        "    test_dir = os.path.join(DATA_DIR, \"test\")\n",
        "\n",
        "print(f\"Train dir: {train_dir}\")\n",
        "print(f\"Val dir: {val_dir}\")\n",
        "print(f\"Test dir: {test_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple dataset class\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data_root, transform, max_per_identity=None):\n",
        "        self.transform = transform\n",
        "        self.samples = []  # (path, identity_name)\n",
        "        \n",
        "        for identity in sorted(os.listdir(data_root)):\n",
        "            identity_path = os.path.join(data_root, identity)\n",
        "            if not os.path.isdir(identity_path): continue\n",
        "            \n",
        "            imgs = [f for f in os.listdir(identity_path) if f.lower().endswith(('.jpg', '.png'))]\n",
        "            if max_per_identity:\n",
        "                imgs = imgs[:max_per_identity]\n",
        "            \n",
        "            for img in imgs:\n",
        "                self.samples.append((os.path.join(identity_path, img), identity))\n",
        "    \n",
        "    def __len__(self): return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        path, identity = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img, identity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract train embeddings (prototypes)\n",
        "transform = get_val_transforms(image_size=160)  # FaceNet uses 160x160\n",
        "train_dataset = SimpleDataset(train_dir, transform, max_per_identity=5)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "\n",
        "# Extract embeddings\n",
        "identity_embeddings = defaultdict(list)\n",
        "\n",
        "print(\"Extracting train embeddings...\")\n",
        "with torch.no_grad():\n",
        "    for images, identities in tqdm(train_loader):\n",
        "        images = images.to(device)\n",
        "        embeddings = model(images)  # [B, 128]\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "        \n",
        "        for emb, identity in zip(embeddings, identities):\n",
        "            identity_embeddings[identity].append(emb)\n",
        "\n",
        "# Compute prototypes (mean embeddings)\n",
        "prototypes = {}\n",
        "for identity, embs in identity_embeddings.items():\n",
        "    mean_emb = np.mean(embs, axis=0)\n",
        "    mean_emb = mean_emb / np.linalg.norm(mean_emb)  # L2 normalize\n",
        "    prototypes[identity] = mean_emb\n",
        "\n",
        "print(f\"Built {len(prototypes)} identity prototypes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to matrix for fast computation\n",
        "identity_list = sorted(prototypes.keys())\n",
        "identity_to_idx = {name: i for i, name in enumerate(identity_list)}\n",
        "prototype_matrix = np.array([prototypes[name] for name in identity_list])  # [N, 128]\n",
        "\n",
        "print(f\"Prototype matrix: {prototype_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Evaluate with Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load eval dataset\n",
        "eval_dir = test_dir if os.path.exists(test_dir) else val_dir\n",
        "eval_dataset = SimpleDataset(eval_dir, transform)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Eval dir: {eval_dir}\")\n",
        "print(f\"Eval samples: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate using cosine similarity\n",
        "all_true_labels = []\n",
        "all_pred_labels = []\n",
        "all_similarities = []\n",
        "\n",
        "print(\"Evaluating with cosine similarity...\")\n",
        "with torch.no_grad():\n",
        "    for images, identities in tqdm(eval_loader):\n",
        "        images = images.to(device)\n",
        "        embeddings = model(images).cpu().numpy()  # [B, 128]\n",
        "        \n",
        "        # Cosine similarity with all prototypes\n",
        "        similarities = np.dot(embeddings, prototype_matrix.T)  # [B, N]\n",
        "        \n",
        "        # Top-1 predictions\n",
        "        pred_indices = np.argmax(similarities, axis=1)\n",
        "        \n",
        "        for identity, pred_idx, sim_row in zip(identities, pred_indices, similarities):\n",
        "            true_idx = identity_to_idx.get(identity, -1)\n",
        "            if true_idx >= 0:\n",
        "                all_true_labels.append(true_idx)\n",
        "                all_pred_labels.append(pred_idx)\n",
        "                all_similarities.append(sim_row)\n",
        "\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "all_pred_labels = np.array(all_pred_labels)\n",
        "all_similarities = np.array(all_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute metrics\n",
        "# Top-1 Accuracy\n",
        "top1_acc = (all_pred_labels == all_true_labels).mean() * 100\n",
        "\n",
        "# Top-5 Accuracy\n",
        "top5_preds = np.argsort(all_similarities, axis=1)[:, -5:]\n",
        "top5_correct = [t in p for t, p in zip(all_true_labels, top5_preds)]\n",
        "top5_acc = np.mean(top5_correct) * 100\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"FaceNet EVALUATION RESULTS\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Total samples: {len(all_true_labels)}\")\n",
        "print(f\"Top-1 Accuracy: {top1_acc:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {top5_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix (top 20 classes)\n",
        "sample_classes = 20\n",
        "class_counts = np.bincount(all_true_labels, minlength=len(identity_list))\n",
        "top_classes = np.argsort(class_counts)[-sample_classes:]\n",
        "mask = np.isin(all_true_labels, top_classes)\n",
        "\n",
        "labels_sub = all_true_labels[mask]\n",
        "preds_sub = all_pred_labels[mask]\n",
        "\n",
        "# Remap\n",
        "label_map = {old: new for new, old in enumerate(sorted(set(labels_sub)))}\n",
        "labels_re = np.array([label_map.get(l, -1) for l in labels_sub])\n",
        "preds_re = np.array([label_map.get(p, -1) for p in preds_sub])\n",
        "valid = (labels_re >= 0) & (preds_re >= 0)\n",
        "\n",
        "cm = confusion_matrix(labels_re[valid], preds_re[valid])\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, cmap='Blues')\n",
        "plt.title(f'Confusion Matrix (Top {sample_classes} Classes)')\n",
        "plt.savefig('/kaggle/working/facenet_confusion_matrix.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curve\n",
        "print(\"Computing ROC Curve...\")\n",
        "\n",
        "sample_size = min(5000, len(all_true_labels))\n",
        "idx = np.random.choice(len(all_true_labels), sample_size, replace=False)\n",
        "labels_sample = all_true_labels[idx]\n",
        "sims_sample = all_similarities[idx]\n",
        "\n",
        "classes = np.unique(labels_sample)\n",
        "y_true_bin = label_binarize(labels_sample, classes=classes)\n",
        "sims_for_classes = sims_sample[:, classes]\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true_bin.ravel(), sims_for_classes.ravel())\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.4f})', color='blue', lw=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('FaceNet ROC Curve')\n",
        "plt.legend()\n",
        "plt.savefig('/kaggle/working/facenet_roc_curve.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"AUC: {roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Threshold Analysis\n",
        "max_sims = np.max(all_similarities, axis=1)\n",
        "is_correct = (all_pred_labels == all_true_labels).astype(int)\n",
        "\n",
        "thresholds = np.arange(0.0, 1.0, 0.05)\n",
        "accs, covs = [], []\n",
        "for t in thresholds:\n",
        "    m = max_sims >= t\n",
        "    covs.append(m.mean() * 100)\n",
        "    accs.append(is_correct[m].mean() * 100 if m.sum() > 0 else 0)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "ax1.plot(thresholds, accs, 'b-', lw=2, label='Accuracy')\n",
        "ax1.set_ylabel('Accuracy (%)', color='blue')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(thresholds, covs, 'r--', lw=2, label='Coverage')\n",
        "ax2.set_ylabel('Coverage (%)', color='red')\n",
        "plt.title('FaceNet: Accuracy vs Coverage')\n",
        "plt.savefig('/kaggle/working/facenet_threshold_analysis.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Final Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'model': 'FaceNet',\n",
        "    'method': 'embedding-based (cosine similarity)',\n",
        "    'embedding_size': 128,\n",
        "    'metrics': {\n",
        "        'top1_accuracy': float(top1_acc),\n",
        "        'top5_accuracy': float(top5_acc),\n",
        "        'auc': float(roc_auc)\n",
        "    },\n",
        "    'eval_samples': int(len(all_true_labels)),\n",
        "    'num_identities': int(len(identity_list))\n",
        "}\n",
        "\n",
        "with open('/kaggle/working/facenet_evaluation_report.json', 'w') as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FACENET FINAL REPORT\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Top-1 Accuracy: {top1_acc:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {top5_acc:.2f}%\")\n",
        "print(f\"AUC: {roc_auc:.4f}\")\n",
        "print(f\"\\nReport saved to: facenet_evaluation_report.json\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
    "language_info": {"name": "python", "version": "3.10.0"}
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
