{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ArcFace Evaluation - Kaggle (Fixed)\n",
        "\n",
        "**FIX**: Dung embedding-based classification thay vi logits-based\n",
        "\n",
        "## Van de truoc do:\n",
        "- ArcFace model can labels de tinh logits (ArcMarginProduct)\n",
        "- Label mapping khac nhau giua train va eval datasets\n",
        "- Ket qua: accuracy gan 0%\n",
        "\n",
        "## Fix:\n",
        "- Extract embeddings cho train set (prototypes)\n",
        "- Extract embeddings cho eval set\n",
        "- Classification bang cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:08:04.691202Z",
          "iopub.status.busy": "2025-12-19T09:08:04.690966Z",
          "iopub.status.idle": "2025-12-19T09:08:04.704982Z",
          "shell.execute_reply": "2025-12-19T09:08:04.704204Z",
          "shell.execute_reply.started": "2025-12-19T09:08:04.691153Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os, sys, time, json\n",
        "import shutil, glob\n",
        "from datetime import datetime\n",
        "# numpy và matplotlib sẽ được import sau khi fix compatibility\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "ROOT = \"/kaggle/working/FaceRecognition\"\n",
        "CHECKPOINT_DIR = \"/kaggle/working/checkpoints/arcface\"\n",
        "KAGGLE_DATASET_NAME = \"celeba-aligned-balanced\"\n",
        "DATA_DIR = f\"/kaggle/input/{KAGGLE_DATASET_NAME}\"\n",
        "CHECKPOINT_DATASET_NAME = \"arcface-checkpoints\"\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:08:04.706309Z",
          "iopub.status.busy": "2025-12-19T09:08:04.706070Z",
          "iopub.status.idle": "2025-12-19T09:08:04.726446Z",
          "shell.execute_reply": "2025-12-19T09:08:04.725730Z",
          "shell.execute_reply.started": "2025-12-19T09:08:04.706286Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoints: ['arcface_best.pth']\n"
          ]
        }
      ],
      "source": [
        "# Copy checkpoint\n",
        "checkpoint_input = f\"/kaggle/input/{CHECKPOINT_DATASET_NAME}\"\n",
        "if os.path.exists(checkpoint_input):\n",
        "    for pth in glob.glob(os.path.join(checkpoint_input, \"**/*.pth\"), recursive=True):\n",
        "        dest = os.path.join(CHECKPOINT_DIR, os.path.basename(pth))\n",
        "        if not os.path.exists(dest): shutil.copy(pth, dest)\n",
        "    print(f\"Checkpoints: {os.listdir(CHECKPOINT_DIR)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cài đặt dependencies với phiên bản tương thích\n",
        "# CHẠY CELL NÀY TRƯỚC KHI IMPORT NUMPY/SCIPY/SKLEARN\n",
        "# Nếu gặp lỗi AttributeError với numpy, restart kernel và chạy lại cell này\n",
        "!pip uninstall -y numpy scipy scikit-learn 2>/dev/null || true\n",
        "!pip cache purge 2>/dev/null || true\n",
        "!pip install -q --no-cache-dir \"numpy>=1.24,<2.0\"\n",
        "!pip install -q --no-cache-dir \"scipy>=1.10,<2.0\"\n",
        "!pip install -q --no-cache-dir \"scikit-learn>=1.3,<2.0\"\n",
        "!pip install -q \"matplotlib>=3.7\" \"seaborn>=0.12\"\n",
        "print(\"[OK] Dependencies installed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:08:04.727399Z",
          "iopub.status.busy": "2025-12-19T09:08:04.727139Z",
          "iopub.status.idle": "2025-12-19T09:08:06.726898Z",
          "shell.execute_reply": "2025-12-19T09:08:06.725998Z",
          "shell.execute_reply.started": "2025-12-19T09:08:04.727377Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Da lay GITHUB_TOKEN\n",
            "Repository da ton tai, dang pull updates...\n",
            "/kaggle/working/FaceRecognition\n",
            "From https://github.com/sin0235/FaceRecognition\n",
            " * branch            fix/lbph-module -> FETCH_HEAD\n",
            "Committer identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@18c6c2083f71.(none)')\n",
            "\n",
            "Working directory: /kaggle/working/FaceRecognition\n",
            "total 140\n",
            "drwxr-xr-x 15 root root  4096 Dec 19 09:03 .\n",
            "drwxr-xr-x  5 root root  4096 Dec 19 09:03 ..\n",
            "drwxr-xr-x  2 root root  4096 Dec 19 09:03 app\n",
            "drwxr-xr-x  2 root root  4096 Dec 19 09:03 configs\n",
            "drwxr-xr-x  8 root root  4096 Dec 19 09:08 .git\n",
            "drwxr-xr-x  3 root root  4096 Dec 19 09:03 .github\n",
            "-rw-r--r--  1 root root  1246 Dec 19 09:03 .gitignore\n",
            "drwxr-xr-x  2 root root  4096 Dec 19 09:03 inference\n",
            "-rw-r--r--  1 root root  1075 Dec 19 09:03 LICENSE\n",
            "drwxr-xr-x  3 root root  4096 Dec 19 09:03 logs\n",
            "drwxr-xr-x  6 root root  4096 Dec 19 09:03 models\n",
            "drwxr-xr-x  2 root root  4096 Dec 19 09:03 notebooks\n",
            "drwxr-xr-x  2 root root  4096 Dec 19 09:03 preprocessing\n",
            "-rw-r--r--  1 root root    59 Dec 19 09:03 README.md\n",
            "-rw-r--r--  1 root root   511 Dec 19 09:03 requirements-colab.txt\n",
            "-rw-r--r--  1 root root  1002 Dec 19 09:03 requirements.txt\n",
            "drwxr-xr-x  2 root root  4096 Dec 19 09:03 scripts\n",
            "drwxr-xr-x  4 root root  4096 Dec 19 09:03 static\n",
            "-rw-r--r--  1 root root 19984 Dec 19 09:03 task.md\n",
            "drwxr-xr-x  2 root root  4096 Dec 19 09:03 templates\n",
            "drwxr-xr-x  2 root root  4096 Dec 19 09:03 uploads\n",
            "-rw-r--r--  1 root root 37504 Dec 19 09:03 web_app.py\n",
            "\n",
            "[OK] Added /kaggle/working/FaceRecognition to sys.path\n"
          ]
        }
      ],
      "source": [
        "# Cau hinh GitHub token\n",
        "try:\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    user_secrets = UserSecretsClient()\n",
        "    GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
        "    print(\"[OK] Da lay GITHUB_TOKEN\")\n",
        "except Exception as e:\n",
        "    GITHUB_TOKEN = None\n",
        "    print(\"[INFO] Su dung public URL\")\n",
        "\n",
        "if GITHUB_TOKEN:\n",
        "    REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/sin0235/FaceRecognition.git\"\n",
        "else:\n",
        "    REPO_URL = \"https://github.com/sin0235/FaceRecognition.git\"\n",
        "\n",
        "# Clone repository\n",
        "if os.path.exists(ROOT):\n",
        "    print(\"Repository da ton tai, dang pull updates...\")\n",
        "    %cd {ROOT}\n",
        "    if GITHUB_TOKEN:\n",
        "        !git remote set-url origin {REPO_URL}\n",
        "    !git pull --no-rebase origin fix/lbph-module\n",
        "else:\n",
        "    print(f\"Dang clone repository...\")\n",
        "    !git clone {REPO_URL} {ROOT}\n",
        "    %cd {ROOT}\n",
        "\n",
        "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
        "!ls -la\n",
        "\n",
        "# Thêm ROOT vào sys.path để import modules\n",
        "if ROOT not in sys.path:\n",
        "    sys.path.insert(0, ROOT)\n",
        "    print(f\"\\n[OK] Added {ROOT} to sys.path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:08:35.078612Z",
          "iopub.status.busy": "2025-12-19T09:08:35.078313Z",
          "iopub.status.idle": "2025-12-19T09:08:37.905028Z",
          "shell.execute_reply": "2025-12-19T09:08:37.904294Z",
          "shell.execute_reply.started": "2025-12-19T09:08:35.078581Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] All imports successful\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Suppress TensorBoard warnings\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import các thư viện cần thiết\n",
        "import sys\n",
        "\n",
        "# Import numpy, matplotlib, seaborn, sklearn\n",
        "# LƯU Ý: Nếu gặp lỗi AttributeError với numpy (numpy.ufunc object has no attribute '__module__'),\n",
        "# cần RESTART KERNEL và chạy lại cell cài đặt dependencies trước\n",
        "try:\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "    from sklearn.manifold import TSNE\n",
        "    print(\"[OK] All imports successful\")\n",
        "    print(f\"NumPy version: {np.__version__}\")\n",
        "except (ImportError, AttributeError, TypeError, ValueError) as e:\n",
        "    print(f\"[ERROR] Import failed: {type(e).__name__}: {e}\")\n",
        "    print(\"\\n[GIẢI PHÁP]\")\n",
        "    print(\"1. Restart kernel: Kernel -> Restart Kernel\")\n",
        "    print(\"2. Chạy lại các cell từ đầu, đặc biệt là cell cài đặt dependencies\")\n",
        "    print(\"3. Nếu vẫn lỗi, chạy lệnh sau trong cell mới:\")\n",
        "    print('   !pip uninstall -y numpy scipy scikit-learn && pip install \"numpy>=1.24,<2.0\" \"scipy>=1.10,<2.0\" \"scikit-learn>=1.3,<2.0\"')\n",
        "    print(\"   Sau đó restart kernel và chạy lại\")\n",
        "    raise\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:08:37.906330Z",
          "iopub.status.busy": "2025-12-19T09:08:37.905939Z",
          "iopub.status.idle": "2025-12-19T09:08:57.190680Z",
          "shell.execute_reply": "2025-12-19T09:08:57.190005Z",
          "shell.execute_reply.started": "2025-12-19T09:08:37.906306Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766135325.266615     193 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766135325.314681     193 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766135325.703651     193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766135325.703690     193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766135325.703692     193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766135325.703695     193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] TensorBoard available\n",
            "Project root: /kaggle/working/FaceRecognition\n",
            "Model: 9343 classes\n",
            "Training epochs: 105\n",
            "Best val acc (training): 81.53%\n"
          ]
        }
      ],
      "source": [
        "from models.arcface.arcface_model import ArcFaceModel\n",
        "\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, \"arcface_best.pth\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "num_classes = checkpoint.get('num_classes', 9343)\n",
        "\n",
        "model = ArcFaceModel(num_classes=num_classes, pretrained=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device).eval()\n",
        "\n",
        "print(f\"Model: {num_classes} classes\")\n",
        "print(f\"Training epochs: {checkpoint.get('epoch', 0) + 1}\")\n",
        "print(f\"Best val acc (training): {checkpoint.get('best_val_acc', 0):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Build Reference Database (Train Set Prototypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:08:57.192486Z",
          "iopub.status.busy": "2025-12-19T09:08:57.191814Z",
          "iopub.status.idle": "2025-12-19T09:08:57.205519Z",
          "shell.execute_reply": "2025-12-19T09:08:57.204746Z",
          "shell.execute_reply.started": "2025-12-19T09:08:57.192454Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dir: /kaggle/input/celeba-aligned-balanced/CelebA_Aligned_Balanced/train\n",
            "Val dir: /kaggle/input/celeba-aligned-balanced/CelebA_Aligned_Balanced/val\n",
            "Test dir: /kaggle/input/celeba-aligned-balanced/CelebA_Aligned_Balanced/test\n"
          ]
        }
      ],
      "source": [
        "from models.arcface.arcface_dataloader import get_val_transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Tim data dirs\n",
        "train_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"train\")\n",
        "val_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"val\")\n",
        "test_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"test\")\n",
        "\n",
        "if not os.path.exists(train_dir):\n",
        "    train_dir = os.path.join(DATA_DIR, \"train\")\n",
        "    val_dir = os.path.join(DATA_DIR, \"val\")\n",
        "    test_dir = os.path.join(DATA_DIR, \"test\")\n",
        "\n",
        "print(f\"Train dir: {train_dir}\")\n",
        "print(f\"Val dir: {val_dir}\")\n",
        "print(f\"Test dir: {test_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:08:57.206718Z",
          "iopub.status.busy": "2025-12-19T09:08:57.206491Z",
          "iopub.status.idle": "2025-12-19T09:08:58.097288Z",
          "shell.execute_reply": "2025-12-19T09:08:58.096546Z",
          "shell.execute_reply.started": "2025-12-19T09:08:57.206697Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Dataset class don gian\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data_root, transform, max_per_identity=None):\n",
        "        self.transform = transform\n",
        "        self.samples = []  # (path, identity_name)\n",
        "        \n",
        "        for identity in sorted(os.listdir(data_root)):\n",
        "            identity_path = os.path.join(data_root, identity)\n",
        "            if not os.path.isdir(identity_path): continue\n",
        "            \n",
        "            imgs = [f for f in os.listdir(identity_path) if f.lower().endswith(('.jpg', '.png'))]\n",
        "            if max_per_identity:\n",
        "                imgs = imgs[:max_per_identity]\n",
        "            \n",
        "            for img in imgs:\n",
        "                self.samples.append((os.path.join(identity_path, img), identity))\n",
        "    \n",
        "    def __len__(self): return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        path, identity = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img, identity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:08:58.100147Z",
          "iopub.status.busy": "2025-12-19T09:08:58.099830Z",
          "iopub.status.idle": "2025-12-19T09:11:21.652492Z",
          "shell.execute_reply": "2025-12-19T09:11:21.651657Z",
          "shell.execute_reply.started": "2025-12-19T09:08:58.100125Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 46715\n",
            "Extracting train embeddings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 365/365 [01:09<00:00,  5.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Built 9343 identity prototypes\n"
          ]
        }
      ],
      "source": [
        "# Extract embeddings cho train set (prototypes)\n",
        "transform = get_val_transforms(image_size=112)\n",
        "train_dataset = SimpleDataset(train_dir, transform, max_per_identity=5)  # Max 5 anh/identity de nhanh\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "\n",
        "# Extract embeddings\n",
        "identity_embeddings = defaultdict(list)\n",
        "\n",
        "print(\"Extracting train embeddings...\")\n",
        "with torch.no_grad():\n",
        "    for images, identities in tqdm(train_loader):\n",
        "        images = images.to(device)\n",
        "        embeddings = model.extract_features(images)  # [B, 512]\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "        \n",
        "        for emb, identity in zip(embeddings, identities):\n",
        "            identity_embeddings[identity].append(emb)\n",
        "\n",
        "# Tinh prototype (mean embedding) cho moi identity\n",
        "prototypes = {}\n",
        "for identity, embs in identity_embeddings.items():\n",
        "    mean_emb = np.mean(embs, axis=0)\n",
        "    mean_emb = mean_emb / np.linalg.norm(mean_emb)  # L2 normalize\n",
        "    prototypes[identity] = mean_emb\n",
        "\n",
        "print(f\"Built {len(prototypes)} identity prototypes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:11:21.654114Z",
          "iopub.status.busy": "2025-12-19T09:11:21.653830Z",
          "iopub.status.idle": "2025-12-19T09:11:21.673784Z",
          "shell.execute_reply": "2025-12-19T09:11:21.673196Z",
          "shell.execute_reply.started": "2025-12-19T09:11:21.654082Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prototype matrix: (9343, 512)\n"
          ]
        }
      ],
      "source": [
        "# Chuyen prototypes thanh matrix de tinh nhanh\n",
        "identity_list = sorted(prototypes.keys())\n",
        "identity_to_idx = {name: i for i, name in enumerate(identity_list)}\n",
        "prototype_matrix = np.array([prototypes[name] for name in identity_list])  # [N, 512]\n",
        "\n",
        "print(f\"Prototype matrix: {prototype_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Evaluate with Embedding-based Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:11:21.674854Z",
          "iopub.status.busy": "2025-12-19T09:11:21.674600Z",
          "iopub.status.idle": "2025-12-19T09:12:20.235990Z",
          "shell.execute_reply": "2025-12-19T09:12:20.235323Z",
          "shell.execute_reply.started": "2025-12-19T09:11:21.674828Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval dir: /kaggle/input/celeba-aligned-balanced/CelebA_Aligned_Balanced/test\n",
            "Eval samples: 20387\n"
          ]
        }
      ],
      "source": [
        "# Load eval dataset (val hoac test)\n",
        "eval_dir = test_dir if os.path.exists(test_dir) else val_dir\n",
        "eval_dataset = SimpleDataset(eval_dir, transform)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Eval dir: {eval_dir}\")\n",
        "print(f\"Eval samples: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:12:20.237453Z",
          "iopub.status.busy": "2025-12-19T09:12:20.237097Z",
          "iopub.status.idle": "2025-12-19T09:12:56.664778Z",
          "shell.execute_reply": "2025-12-19T09:12:56.663950Z",
          "shell.execute_reply.started": "2025-12-19T09:12:20.237417Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating with cosine similarity...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 160/160 [00:36<00:00,  4.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation summary:\n",
            "  Total eval samples: 20387\n",
            "  Valid samples (có trong train set): 20387\n",
            "Similarities shape: (20387, 9343)\n",
            "Expected: [N_samples=20387, N_identities=9343]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate bang cosine similarity\n",
        "all_true_labels = []  # index trong identity_list\n",
        "all_pred_labels = []\n",
        "all_similarities = []\n",
        "\n",
        "print(\"Evaluating with cosine similarity...\")\n",
        "with torch.no_grad():\n",
        "    for images, identities in tqdm(eval_loader):\n",
        "        images = images.to(device)\n",
        "        embeddings = model.extract_features(images).cpu().numpy()  # [B, 512]\n",
        "        \n",
        "        # Cosine similarity voi tat ca prototypes\n",
        "        # embeddings: [B, 512], prototype_matrix: [N, 512]\n",
        "        similarities = np.dot(embeddings, prototype_matrix.T)  # [B, N]\n",
        "        \n",
        "        # Top-1 predictions\n",
        "        pred_indices = np.argmax(similarities, axis=1)\n",
        "        \n",
        "        for identity, pred_idx, sim_row in zip(identities, pred_indices, similarities):\n",
        "            true_idx = identity_to_idx.get(identity, -1)\n",
        "            if true_idx >= 0:\n",
        "                all_true_labels.append(true_idx)\n",
        "                all_pred_labels.append(pred_idx)\n",
        "                # Đảm bảo sim_row là 1D array trước khi append\n",
        "                sim_row_flat = np.array(sim_row).flatten()\n",
        "                all_similarities.append(sim_row_flat)\n",
        "\n",
        "# Kiểm tra số lượng samples TRƯỚC KHI convert\n",
        "if len(all_true_labels) == 0:\n",
        "    print(\"\\n[ERROR] Không có samples nào có identity trong train set!\")\n",
        "    print(\"Nguyên nhân: Tất cả identities trong eval set không có trong train set.\")\n",
        "    print(f\"Train set có {len(identity_list)} identities\")\n",
        "    if len(eval_dataset) > 0:\n",
        "        sample_identities = list(set([i for _, i in eval_dataset.samples[:50]]))\n",
        "        print(f\"Sample eval identities (first 50 unique): {sample_identities[:10]}...\")\n",
        "        overlap = len(set(sample_identities) & set(identity_list))\n",
        "        print(f\"Overlap với train set: {overlap} identities\")\n",
        "    raise ValueError(\"Không có samples hợp lệ để evaluate\")\n",
        "\n",
        "print(f\"\\nEvaluation summary:\")\n",
        "print(f\"  Total eval samples: {len(eval_dataset)}\")\n",
        "print(f\"  Valid samples (có trong train set): {len(all_true_labels)}\")\n",
        "\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "all_pred_labels = np.array(all_pred_labels)\n",
        "\n",
        "# Convert all_similarities thành 2D array [N_samples, N_identities]\n",
        "if len(all_similarities) == 0:\n",
        "    raise ValueError(\"Không có similarities nào được tính!\")\n",
        "\n",
        "all_similarities = np.array(all_similarities)\n",
        "\n",
        "# Đảm bảo all_similarities là 2D array\n",
        "if all_similarities.ndim == 1:\n",
        "    # Nếu chỉ có 1 sample, reshape thành [1, N_identities]\n",
        "    all_similarities = all_similarities.reshape(1, -1)\n",
        "elif all_similarities.ndim == 0:\n",
        "    raise ValueError(\"all_similarities is scalar, không hợp lệ\")\n",
        "\n",
        "print(f\"Similarities shape: {all_similarities.shape}\")\n",
        "print(f\"Expected: [N_samples={len(all_true_labels)}, N_identities={len(identity_list)}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:12:56.666282Z",
          "iopub.status.busy": "2025-12-19T09:12:56.665992Z",
          "iopub.status.idle": "2025-12-19T09:13:00.796785Z",
          "shell.execute_reply": "2025-12-19T09:13:00.796032Z",
          "shell.execute_reply.started": "2025-12-19T09:12:56.666252Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "EVALUATION RESULTS (Embedding-based)\n",
            "==================================================\n",
            "Total samples: 20387\n",
            "Top-1 Accuracy: 88.14%\n",
            "Top-5 Accuracy: 94.11%\n"
          ]
        }
      ],
      "source": [
        "# Tinh metrics\n",
        "if len(all_true_labels) == 0:\n",
        "    raise ValueError(\"Không có samples nào để evaluate!\")\n",
        "\n",
        "# Đảm bảo all_similarities là 2D\n",
        "if all_similarities.ndim == 1:\n",
        "    all_similarities = all_similarities.reshape(1, -1)\n",
        "\n",
        "# Top-1 Accuracy\n",
        "if len(all_pred_labels) > 0 and len(all_true_labels) > 0:\n",
        "    top1_acc = (all_pred_labels == all_true_labels).mean() * 100\n",
        "else:\n",
        "    top1_acc = 0.0\n",
        "\n",
        "# Top-5 Accuracy\n",
        "if all_similarities.shape[1] >= 5:\n",
        "    top5_preds = np.argsort(all_similarities, axis=1)[:, -5:]\n",
        "    top5_correct = [t in p for t, p in zip(all_true_labels, top5_preds)]\n",
        "    top5_acc = np.mean(top5_correct) * 100\n",
        "else:\n",
        "    top5_acc = top1_acc\n",
        "    print(f\"[INFO] Chỉ có {all_similarities.shape[1]} identities, Top-5 = Top-1\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"EVALUATION RESULTS (Embedding-based)\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Total samples: {len(all_true_labels)}\")\n",
        "print(f\"Top-1 Accuracy: {top1_acc:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {top5_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:13:00.797981Z",
          "iopub.status.busy": "2025-12-19T09:13:00.797702Z",
          "iopub.status.idle": "2025-12-19T09:13:01.150396Z",
          "shell.execute_reply": "2025-12-19T09:13:01.149741Z",
          "shell.execute_reply.started": "2025-12-19T09:13:00.797946Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix (top 20 classes)\n",
        "sample_classes = 20\n",
        "class_counts = np.bincount(all_true_labels, minlength=len(identity_list))\n",
        "top_classes = np.argsort(class_counts)[-sample_classes:]\n",
        "mask = np.isin(all_true_labels, top_classes)\n",
        "\n",
        "labels_sub = all_true_labels[mask]\n",
        "preds_sub = all_pred_labels[mask]\n",
        "\n",
        "# Remap\n",
        "label_map = {old: new for new, old in enumerate(sorted(set(labels_sub)))}\n",
        "labels_re = np.array([label_map.get(l, -1) for l in labels_sub])\n",
        "preds_re = np.array([label_map.get(p, -1) for p in preds_sub])\n",
        "valid = (labels_re >= 0) & (preds_re >= 0)\n",
        "\n",
        "cm = confusion_matrix(labels_re[valid], preds_re[valid])\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, cmap='Blues')\n",
        "plt.title(f'Confusion Matrix (Top {sample_classes} Classes)')\n",
        "plt.savefig('/kaggle/working/confusion_matrix.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:13:01.152358Z",
          "iopub.status.busy": "2025-12-19T09:13:01.151484Z",
          "iopub.status.idle": "2025-12-19T09:13:01.401539Z",
          "shell.execute_reply": "2025-12-19T09:13:01.400846Z",
          "shell.execute_reply.started": "2025-12-19T09:13:01.152331Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing ROC Curve...\n",
            "AUC: 0.9595\n",
            "EER: 0.1013 (threshold: 0.563)\n"
          ]
        }
      ],
      "source": [
        "# ROC Curve - Binary classification: correct vs incorrect prediction\n",
        "print(\"Computing ROC Curve...\")\n",
        "\n",
        "# Sample\n",
        "sample_size = min(5000, len(all_true_labels))\n",
        "idx = np.random.choice(len(all_true_labels), sample_size, replace=False)\n",
        "labels_sample = all_true_labels[idx]\n",
        "sims_sample = all_similarities[idx]\n",
        "\n",
        "preds_sample = all_pred_labels[idx]\n",
        "\n",
        "# Binary labels: 1 = correct prediction, 0 = incorrect\n",
        "y_true_binary = (labels_sample == preds_sample).astype(int)\n",
        "\n",
        "# Scores: similarity to true class\n",
        "y_scores = []\n",
        "for i, true_idx in enumerate(labels_sample):\n",
        "    y_scores.append(sims_sample[i, true_idx])\n",
        "\n",
        "y_scores = np.array(y_scores)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true_binary, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# EER (Equal Error Rate)\n",
        "eer_idx = np.argmin(np.abs(fpr - (1 - tpr)))\n",
        "eer = fpr[eer_idx]\n",
        "eer_threshold = thresholds[eer_idx]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.4f})', color='blue', lw=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.scatter([eer], [1-eer], color='red', s=100, zorder=5, \n",
        "           label=f'EER = {eer:.4f} (thresh={eer_threshold:.3f})')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ArcFace ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig('/kaggle/working/roc_curve.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"AUC: {roc_auc:.4f}\")\n",
        "print(f\"EER: {eer:.4f} (threshold: {eer_threshold:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:13:01.402691Z",
          "iopub.status.busy": "2025-12-19T09:13:01.402456Z",
          "iopub.status.idle": "2025-12-19T09:13:01.663535Z",
          "shell.execute_reply": "2025-12-19T09:13:01.662695Z",
          "shell.execute_reply.started": "2025-12-19T09:13:01.402670Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Threshold Analysis\n",
        "max_sims = np.max(all_similarities, axis=1)\n",
        "is_correct = (all_pred_labels == all_true_labels).astype(int)\n",
        "\n",
        "thresholds = np.arange(0.0, 1.0, 0.05)\n",
        "accs, covs = [], []\n",
        "for t in thresholds:\n",
        "    m = max_sims >= t\n",
        "    covs.append(m.mean() * 100)\n",
        "    accs.append(is_correct[m].mean() * 100 if m.sum() > 0 else 0)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "ax1.plot(thresholds, accs, 'b-', lw=2, label='Accuracy')\n",
        "ax1.set_ylabel('Accuracy (%)', color='blue')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(thresholds, covs, 'r--', lw=2, label='Coverage')\n",
        "ax2.set_ylabel('Coverage (%)', color='red')\n",
        "plt.title('Accuracy vs Coverage at Thresholds')\n",
        "plt.savefig('/kaggle/working/threshold_analysis.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Performance Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:13:01.665016Z",
          "iopub.status.busy": "2025-12-19T09:13:01.664704Z",
          "iopub.status.idle": "2025-12-19T09:13:02.490217Z",
          "shell.execute_reply": "2025-12-19T09:13:02.489350Z",
          "shell.execute_reply.started": "2025-12-19T09:13:01.664986Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latency: 7.02 ms (avg)\n"
          ]
        }
      ],
      "source": [
        "# Latency Test\n",
        "dummy = torch.randn(1, 3, 112, 112).to(device)\n",
        "for _ in range(10): model.extract_features(dummy)\n",
        "\n",
        "latencies = []\n",
        "for _ in range(100):\n",
        "    if device == 'cuda': torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    model.extract_features(torch.randn(1, 3, 112, 112).to(device))\n",
        "    if device == 'cuda': torch.cuda.synchronize()\n",
        "    latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "print(f\"Latency: {np.mean(latencies):.2f} ms (avg)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:13:02.491381Z",
          "iopub.status.busy": "2025-12-19T09:13:02.491132Z",
          "iopub.status.idle": "2025-12-19T09:13:05.622953Z",
          "shell.execute_reply": "2025-12-19T09:13:05.622355Z",
          "shell.execute_reply.started": "2025-12-19T09:13:02.491359Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Throughput Test\n",
        "batch_sizes = [1, 16, 32, 64, 128]\n",
        "throughputs = []\n",
        "for bs in batch_sizes:\n",
        "    dummy = torch.randn(bs, 3, 112, 112).to(device)\n",
        "    for _ in range(5): model.extract_features(dummy)\n",
        "    if device == 'cuda': torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(20): model.extract_features(dummy)\n",
        "    if device == 'cuda': torch.cuda.synchronize()\n",
        "    throughputs.append((bs * 20) / (time.time() - start))\n",
        "\n",
        "plt.bar([str(b) for b in batch_sizes], throughputs)\n",
        "plt.ylabel('Throughput (img/s)')\n",
        "plt.savefig('/kaggle/working/throughput.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. t-SNE Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:13:05.624621Z",
          "iopub.status.busy": "2025-12-19T09:13:05.624026Z",
          "iopub.status.idle": "2025-12-19T09:13:08.764203Z",
          "shell.execute_reply": "2025-12-19T09:13:08.763443Z",
          "shell.execute_reply.started": "2025-12-19T09:13:05.624587Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 16/160 [00:02<00:18,  7.99it/s]\n",
            "Exception ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x7d7b0c4ebec0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n",
            "    self._make_controller_from_path(filepath)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/threadpoolctl.py\", line 1187, in _make_controller_from_path\n",
            "    lib_controller = controller_class(\n",
            "                     ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/threadpoolctl.py\", line 114, in __init__\n",
            "    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OSError: /usr/local/lib/python3.12/dist-packages/numpy.libs/libscipy_openblas64_-fdde5778.so: cannot open shared object file: No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t-SNE samples: 208\n"
          ]
        }
      ],
      "source": [
        "# Extract eval embeddings for t-SNE\n",
        "emb_list, lbl_list = [], []\n",
        "with torch.no_grad():\n",
        "    for img, identity in tqdm(eval_loader):\n",
        "        if len(emb_list) * 128 >= 2000: break\n",
        "        emb_list.append(model.extract_features(img.to(device)).cpu().numpy())\n",
        "        lbl_list.extend([identity_to_idx.get(i, -1) for i in identity])\n",
        "\n",
        "embs = np.concatenate(emb_list)[:2000]\n",
        "lbls = np.array(lbl_list)[:2000]\n",
        "valid = lbls >= 0\n",
        "embs, lbls = embs[valid], lbls[valid]\n",
        "\n",
        "# Top 50 classes\n",
        "uniq = np.unique(lbls)[:50]\n",
        "mask = np.isin(lbls, uniq)\n",
        "embs, lbls = embs[mask], lbls[mask]\n",
        "\n",
        "print(f\"t-SNE samples: {len(embs)}\")\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "embs_2d = tsne.fit_transform(embs)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(embs_2d[:, 0], embs_2d[:, 1], c=lbls, cmap='tab20', s=10, alpha=0.6)\n",
        "plt.title('t-SNE Embedding')\n",
        "plt.savefig('/kaggle/working/tsne.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Final Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:13:08.765613Z",
          "iopub.status.busy": "2025-12-19T09:13:08.765347Z",
          "iopub.status.idle": "2025-12-19T09:13:08.774300Z",
          "shell.execute_reply": "2025-12-19T09:13:08.773669Z",
          "shell.execute_reply.started": "2025-12-19T09:13:08.765584Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "FINAL EVALUATION REPORT\n",
            "==================================================\n",
            "Method: Embedding-based (cosine similarity)\n",
            "Top-1 Accuracy: 88.14%\n",
            "Top-5 Accuracy: 94.11%\n",
            "AUC: 0.9595\n",
            "EER: 0.1013 (threshold: 0.563)\n",
            "Avg Latency: 7.02 ms\n"
          ]
        }
      ],
      "source": [
        "report = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'method': 'embedding-based (cosine similarity)',\n",
        "    'model': {\n",
        "        'num_classes': int(num_classes),\n",
        "        'epochs': int(checkpoint.get('epoch', 0) + 1),\n",
        "        'training_val_acc': float(checkpoint.get('best_val_acc', 0))\n",
        "    },\n",
        "    'metrics': {\n",
        "        'top1_accuracy': float(top1_acc),\n",
        "        'top5_accuracy': float(top5_acc),\n",
        "        'auc': float(roc_auc),\n",
        "        'eer': float(eer) if 'eer' in globals() else None,\n",
        "        'eer_threshold': float(eer_threshold) if 'eer_threshold' in globals() else None\n",
        "    },\n",
        "    'performance': {\n",
        "        'avg_latency_ms': float(np.mean(latencies)),\n",
        "        'max_throughput': float(max(throughputs))\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('/kaggle/working/evaluation_report.json', 'w') as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL EVALUATION REPORT\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Method: Embedding-based (cosine similarity)\")\n",
        "print(f\"Top-1 Accuracy: {top1_acc:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {top5_acc:.2f}%\")\n",
        "print(f\"AUC: {roc_auc:.4f}\")\n",
        "if 'eer' in globals():\n",
        "    print(f\"EER: {eer:.4f} (threshold: {eer_threshold:.3f})\")\n",
        "print(f\"Avg Latency: {np.mean(latencies):.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:13:08.775830Z",
          "iopub.status.busy": "2025-12-19T09:13:08.775445Z",
          "iopub.status.idle": "2025-12-19T09:13:08.958735Z",
          "shell.execute_reply": "2025-12-19T09:13:08.957951Z",
          "shell.execute_reply.started": "2025-12-19T09:13:08.775797Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 43030 Dec 19 09:13 /kaggle/working/confusion_matrix.png\n",
            "-rw-r--r-- 1 root root   511 Dec 19 09:13 /kaggle/working/evaluation_report.json\n",
            "-rw-r--r-- 1 root root 62806 Dec 19 09:13 /kaggle/working/roc_curve.png\n",
            "-rw-r--r-- 1 root root 57329 Dec 19 09:13 /kaggle/working/threshold_analysis.png\n",
            "-rw-r--r-- 1 root root 45882 Dec 19 09:13 /kaggle/working/throughput.png\n",
            "-rw-r--r-- 1 root root 80808 Dec 19 09:13 /kaggle/working/tsne.png\n"
          ]
        }
      ],
      "source": [
        "!ls -la /kaggle/working/*.png /kaggle/working/*.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-19T09:25:27.532413Z",
          "iopub.status.busy": "2025-12-19T09:25:27.531973Z",
          "iopub.status.idle": "2025-12-19T09:25:27.738707Z",
          "shell.execute_reply": "2025-12-19T09:25:27.737726Z",
          "shell.execute_reply.started": "2025-12-19T09:25:27.532386Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã lưu artifacts vào thư mục: /kaggle/working/arcface_eval_artifacts\n",
            "Đã tạo file zip: /kaggle/working/arcface_eval_artifacts.zip\n",
            "-rw-r--r-- 1 root root 248K Dec 19 09:25 /kaggle/working/arcface_eval_artifacts.zip\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "base_dir = \"/kaggle/working\"\n",
        "artifacts_dir = os.path.join(base_dir, \"arcface_eval_artifacts\")\n",
        "os.makedirs(artifacts_dir, exist_ok=True)\n",
        "\n",
        "# Lưu các dữ liệu nhẹ phục vụ trực quan báo cáo\n",
        "# 1. ROC curve (fpr, tpr)\n",
        "roc_df = pd.DataFrame({\n",
        "    \"fpr\": np.array(fpr, dtype=float),\n",
        "    \"tpr\": np.array(tpr, dtype=float),\n",
        "})\n",
        "roc_path = os.path.join(artifacts_dir, \"roc_curve_data.csv\")\n",
        "roc_df.to_csv(roc_path, index=False)\n",
        "\n",
        "# 2. Threshold analysis (threshold, accuracy, coverage)\n",
        "thr_df = pd.DataFrame({\n",
        "    \"threshold\": np.array(thresholds, dtype=float),\n",
        "    \"accuracy\": np.array(accs, dtype=float),\n",
        "    \"coverage\": np.array(covs, dtype=float),\n",
        "})\n",
        "thr_path = os.path.join(artifacts_dir, \"threshold_analysis_data.csv\")\n",
        "thr_df.to_csv(thr_path, index=False)\n",
        "\n",
        "# 3. t-SNE 2D embeddings (giới hạn ~2000 điểm như trên)\n",
        "if \"embs_2d\" in globals() and \"lbls\" in globals():\n",
        "    tsne_df = pd.DataFrame({\n",
        "        \"x\": embs_2d[:, 0].astype(float),\n",
        "        \"y\": embs_2d[:, 1].astype(float),\n",
        "        \"label_idx\": lbls.astype(int),\n",
        "    })\n",
        "    tsne_path = os.path.join(artifacts_dir, \"tsne_points.csv\")\n",
        "    tsne_df.to_csv(tsne_path, index=False)\n",
        "\n",
        "# 4. Confusion matrix giá trị số + tên lớp\n",
        "# Sử dụng lại biến từ cell vẽ confusion matrix, tránh lệch kích thước với mask của t-SNE\n",
        "cm_path = os.path.join(artifacts_dir, \"confusion_matrix_values.csv\")\n",
        "if \"cm\" in globals():\n",
        "    try:\n",
        "        # labels_sub và valid được tạo ở cell confusion matrix\n",
        "        cm_original_indices = sorted(set(labels_sub[valid]))\n",
        "        cm_class_names = [identity_list[i] for i in cm_original_indices]\n",
        "        cm_df = pd.DataFrame(cm, index=cm_class_names, columns=cm_class_names)\n",
        "        cm_df.to_csv(cm_path)\n",
        "    except Exception as e:\n",
        "        # Fallback: nếu thiếu biến, lưu ma trận với index mặc định\n",
        "        cm_df = pd.DataFrame(cm)\n",
        "        cm_df.to_csv(cm_path)\n",
        "else:\n",
        "    # Nếu chưa có cm (chưa chạy cell confusion matrix) thì bỏ qua phần này\n",
        "    print(\"[WARN] Chưa có confusion matrix trong scope, bỏ qua lưu confusion_matrix_values.csv\")\n",
        "\n",
        "# 5. Copy các file báo cáo đã có (CSV mẫu, ảnh, JSON)\n",
        "existing_files = [\n",
        "    \"eval_results_sample.csv\",\n",
        "    \"confusion_matrix.png\",\n",
        "    \"roc_curve.png\",\n",
        "    \"threshold_analysis.png\",\n",
        "    \"throughput.png\",\n",
        "    \"tsne.png\",\n",
        "    \"evaluation_report.json\",\n",
        "]\n",
        "\n",
        "for fname in existing_files:\n",
        "    src = os.path.join(base_dir, fname)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy(src, os.path.join(artifacts_dir, fname))\n",
        "\n",
        "# 6. Đóng gói zip để tải nhanh\n",
        "zip_path = os.path.join(base_dir, \"arcface_eval_artifacts.zip\")\n",
        "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    for root, _, files in os.walk(artifacts_dir):\n",
        "        for f in files:\n",
        "            full_path = os.path.join(root, f)\n",
        "            rel_path = os.path.relpath(full_path, artifacts_dir)\n",
        "            zf.write(full_path, arcname=rel_path)\n",
        "\n",
        "print(f\"Đã lưu artifacts vào thư mục: {artifacts_dir}\")\n",
        "print(f\"Đã tạo file zip: {zip_path}\")\n",
        "\n",
        "!ls -lh /kaggle/working/arcface_eval_artifacts.zip"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 15014702,
          "datasetId": 9069948,
          "sourceId": 14218639,
          "sourceType": "datasetVersion"
        },
        {
          "databundleVersionId": 15016901,
          "datasetId": 9007284,
          "sourceId": 14220681,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31236,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
