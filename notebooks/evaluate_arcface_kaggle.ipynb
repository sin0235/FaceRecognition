{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ArcFace Evaluation - Kaggle (Fixed)\n",
        "\n",
        "**FIX**: Dung embedding-based classification thay vi logits-based\n",
        "\n",
        "## Van de truoc do:\n",
        "- ArcFace model can labels de tinh logits (ArcMarginProduct)\n",
        "- Label mapping khac nhau giua train va eval datasets\n",
        "- Ket qua: accuracy gan 0%\n",
        "\n",
        "## Fix:\n",
        "- Extract embeddings cho train set (prototypes)\n",
        "- Extract embeddings cho eval set\n",
        "- Classification bang cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, time, json\n",
        "import shutil, glob\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "ROOT = \"/kaggle/working/FaceRecognition\"\n",
        "CHECKPOINT_DIR = \"/kaggle/working/checkpoints/arcface\"\n",
        "KAGGLE_DATASET_NAME = \"celeba-aligned-balanced\"\n",
        "DATA_DIR = f\"/kaggle/input/{KAGGLE_DATASET_NAME}\"\n",
        "CHECKPOINT_DATASET_NAME = \"arcface-checkpoints\"\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy checkpoint\n",
        "checkpoint_input = f\"/kaggle/input/{CHECKPOINT_DATASET_NAME}\"\n",
        "if os.path.exists(checkpoint_input):\n",
        "    for pth in glob.glob(os.path.join(checkpoint_input, \"**/*.pth\"), recursive=True):\n",
        "        dest = os.path.join(CHECKPOINT_DIR, os.path.basename(pth))\n",
        "        if not os.path.exists(dest): shutil.copy(pth, dest)\n",
        "    print(f\"Checkpoints: {os.listdir(CHECKPOINT_DIR)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo\n",
        "REPO_URL = \"https://github.com/sin0235/FaceRecognition.git\"\n",
        "if os.path.exists(ROOT):\n",
        "    %cd {ROOT}\n",
        "    !git pull\n",
        "else:\n",
        "    !git clone {REPO_URL} {ROOT}\n",
        "    %cd {ROOT}\n",
        "if ROOT not in sys.path: sys.path.insert(0, ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q opencv-python-headless Pillow scikit-learn tqdm pyyaml matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.arcface.arcface_model import ArcFaceModel\n",
        "\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, \"arcface_best.pth\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "num_classes = checkpoint.get('num_classes', 9343)\n",
        "\n",
        "model = ArcFaceModel(num_classes=num_classes, pretrained=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device).eval()\n",
        "\n",
        "print(f\"Model: {num_classes} classes\")\n",
        "print(f\"Training epochs: {checkpoint.get('epoch', 0) + 1}\")\n",
        "print(f\"Best val acc (training): {checkpoint.get('best_val_acc', 0):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Build Reference Database (Train Set Prototypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.arcface.arcface_dataloader import get_val_transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Tim data dirs\n",
        "train_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"train\")\n",
        "val_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"val\")\n",
        "test_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"test\")\n",
        "\n",
        "if not os.path.exists(train_dir):\n",
        "    train_dir = os.path.join(DATA_DIR, \"train\")\n",
        "    val_dir = os.path.join(DATA_DIR, \"val\")\n",
        "    test_dir = os.path.join(DATA_DIR, \"test\")\n",
        "\n",
        "print(f\"Train dir: {train_dir}\")\n",
        "print(f\"Val dir: {val_dir}\")\n",
        "print(f\"Test dir: {test_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset class don gian\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data_root, transform, max_per_identity=None):\n",
        "        self.transform = transform\n",
        "        self.samples = []  # (path, identity_name)\n",
        "        \n",
        "        for identity in sorted(os.listdir(data_root)):\n",
        "            identity_path = os.path.join(data_root, identity)\n",
        "            if not os.path.isdir(identity_path): continue\n",
        "            \n",
        "            imgs = [f for f in os.listdir(identity_path) if f.lower().endswith(('.jpg', '.png'))]\n",
        "            if max_per_identity:\n",
        "                imgs = imgs[:max_per_identity]\n",
        "            \n",
        "            for img in imgs:\n",
        "                self.samples.append((os.path.join(identity_path, img), identity))\n",
        "    \n",
        "    def __len__(self): return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        path, identity = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img, identity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract embeddings cho train set (prototypes)\n",
        "transform = get_val_transforms(image_size=112)\n",
        "train_dataset = SimpleDataset(train_dir, transform, max_per_identity=5)  # Max 5 anh/identity de nhanh\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "\n",
        "# Extract embeddings\n",
        "identity_embeddings = defaultdict(list)\n",
        "\n",
        "print(\"Extracting train embeddings...\")\n",
        "with torch.no_grad():\n",
        "    for images, identities in tqdm(train_loader):\n",
        "        images = images.to(device)\n",
        "        embeddings = model.extract_features(images)  # [B, 512]\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "        \n",
        "        for emb, identity in zip(embeddings, identities):\n",
        "            identity_embeddings[identity].append(emb)\n",
        "\n",
        "# Tinh prototype (mean embedding) cho moi identity\n",
        "prototypes = {}\n",
        "for identity, embs in identity_embeddings.items():\n",
        "    mean_emb = np.mean(embs, axis=0)\n",
        "    mean_emb = mean_emb / np.linalg.norm(mean_emb)  # L2 normalize\n",
        "    prototypes[identity] = mean_emb\n",
        "\n",
        "print(f\"Built {len(prototypes)} identity prototypes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chuyen prototypes thanh matrix de tinh nhanh\n",
        "identity_list = sorted(prototypes.keys())\n",
        "identity_to_idx = {name: i for i, name in enumerate(identity_list)}\n",
        "prototype_matrix = np.array([prototypes[name] for name in identity_list])  # [N, 512]\n",
        "\n",
        "print(f\"Prototype matrix: {prototype_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Evaluate with Embedding-based Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load eval dataset (val hoac test)\n",
        "eval_dir = test_dir if os.path.exists(test_dir) else val_dir\n",
        "eval_dataset = SimpleDataset(eval_dir, transform)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Eval dir: {eval_dir}\")\n",
        "print(f\"Eval samples: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate bang cosine similarity\n",
        "all_true_labels = []  # index trong identity_list\n",
        "all_pred_labels = []\n",
        "all_similarities = []\n",
        "\n",
        "print(\"Evaluating with cosine similarity...\")\n",
        "with torch.no_grad():\n",
        "    for images, identities in tqdm(eval_loader):\n",
        "        images = images.to(device)\n",
        "        embeddings = model.extract_features(images).cpu().numpy()  # [B, 512]\n",
        "        \n",
        "        # Cosine similarity voi tat ca prototypes\n",
        "        # embeddings: [B, 512], prototype_matrix: [N, 512]\n",
        "        similarities = np.dot(embeddings, prototype_matrix.T)  # [B, N]\n",
        "        \n",
        "        # Top-1 predictions\n",
        "        pred_indices = np.argmax(similarities, axis=1)\n",
        "        \n",
        "        for identity, pred_idx, sim_row in zip(identities, pred_indices, similarities):\n",
        "            true_idx = identity_to_idx.get(identity, -1)\n",
        "            if true_idx >= 0:\n",
        "                all_true_labels.append(true_idx)\n",
        "                all_pred_labels.append(pred_idx)\n",
        "                all_similarities.append(sim_row)\n",
        "\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "all_pred_labels = np.array(all_pred_labels)\n",
        "all_similarities = np.array(all_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tinh metrics\n",
        "# Top-1 Accuracy\n",
        "top1_acc = (all_pred_labels == all_true_labels).mean() * 100\n",
        "\n",
        "# Top-5 Accuracy\n",
        "top5_preds = np.argsort(all_similarities, axis=1)[:, -5:]\n",
        "top5_correct = [t in p for t, p in zip(all_true_labels, top5_preds)]\n",
        "top5_acc = np.mean(top5_correct) * 100\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"EVALUATION RESULTS (Embedding-based)\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Total samples: {len(all_true_labels)}\")\n",
        "print(f\"Top-1 Accuracy: {top1_acc:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {top5_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix (top 20 classes)\n",
        "sample_classes = 20\n",
        "class_counts = np.bincount(all_true_labels, minlength=len(identity_list))\n",
        "top_classes = np.argsort(class_counts)[-sample_classes:]\n",
        "mask = np.isin(all_true_labels, top_classes)\n",
        "\n",
        "labels_sub = all_true_labels[mask]\n",
        "preds_sub = all_pred_labels[mask]\n",
        "\n",
        "# Remap\n",
        "label_map = {old: new for new, old in enumerate(sorted(set(labels_sub)))}\n",
        "labels_re = np.array([label_map.get(l, -1) for l in labels_sub])\n",
        "preds_re = np.array([label_map.get(p, -1) for p in preds_sub])\n",
        "valid = (labels_re >= 0) & (preds_re >= 0)\n",
        "\n",
        "cm = confusion_matrix(labels_re[valid], preds_re[valid])\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, cmap='Blues')\n",
        "plt.title(f'Confusion Matrix (Top {sample_classes} Classes)')\n",
        "plt.savefig('/kaggle/working/confusion_matrix.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curve - Binary classification: correct vs incorrect prediction\n",
        "print(\"Computing ROC Curve...\")\n",
        "\n",
        "# Sample\n",
        "sample_size = min(5000, len(all_true_labels))\n",
        "idx = np.random.choice(len(all_true_labels), sample_size, replace=False)\n",
        "labels_sample = all_true_labels[idx]\n",
        "sims_sample = all_similarities[idx]\n",
        "\n",
        "preds_sample = all_pred_labels[idx]\n",
        "\n",
        "# Binary labels: 1 = correct prediction, 0 = incorrect\n",
        "y_true_binary = (labels_sample == preds_sample).astype(int)\n",
        "\n",
        "# Scores: similarity to true class\n",
        "y_scores = []\n",
        "for i, true_idx in enumerate(labels_sample):\n",
        "    y_scores.append(sims_sample[i, true_idx])\n",
        "\n",
        "y_scores = np.array(y_scores)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true_binary, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# EER (Equal Error Rate)\n",
        "eer_idx = np.argmin(np.abs(fpr - (1 - tpr)))\n",
        "eer = fpr[eer_idx]\n",
        "eer_threshold = thresholds[eer_idx]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.4f})', color='blue', lw=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.scatter([eer], [1-eer], color='red', s=100, zorder=5, \n",
        "           label=f'EER = {eer:.4f} (thresh={eer_threshold:.3f})')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ArcFace ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig('/kaggle/working/roc_curve.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"AUC: {roc_auc:.4f}\")\n",
        "print(f\"EER: {eer:.4f} (threshold: {eer_threshold:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Threshold Analysis\n",
        "max_sims = np.max(all_similarities, axis=1)\n",
        "is_correct = (all_pred_labels == all_true_labels).astype(int)\n",
        "\n",
        "thresholds = np.arange(0.0, 1.0, 0.05)\n",
        "accs, covs = [], []\n",
        "for t in thresholds:\n",
        "    m = max_sims >= t\n",
        "    covs.append(m.mean() * 100)\n",
        "    accs.append(is_correct[m].mean() * 100 if m.sum() > 0 else 0)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "ax1.plot(thresholds, accs, 'b-', lw=2, label='Accuracy')\n",
        "ax1.set_ylabel('Accuracy (%)', color='blue')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(thresholds, covs, 'r--', lw=2, label='Coverage')\n",
        "ax2.set_ylabel('Coverage (%)', color='red')\n",
        "plt.title('Accuracy vs Coverage at Thresholds')\n",
        "plt.savefig('/kaggle/working/threshold_analysis.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Performance Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Latency Test\n",
        "dummy = torch.randn(1, 3, 112, 112).to(device)\n",
        "for _ in range(10): model.extract_features(dummy)\n",
        "\n",
        "latencies = []\n",
        "for _ in range(100):\n",
        "    if device == 'cuda': torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    model.extract_features(torch.randn(1, 3, 112, 112).to(device))\n",
        "    if device == 'cuda': torch.cuda.synchronize()\n",
        "    latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "print(f\"Latency: {np.mean(latencies):.2f} ms (avg)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Throughput Test\n",
        "batch_sizes = [1, 16, 32, 64, 128]\n",
        "throughputs = []\n",
        "for bs in batch_sizes:\n",
        "    dummy = torch.randn(bs, 3, 112, 112).to(device)\n",
        "    for _ in range(5): model.extract_features(dummy)\n",
        "    if device == 'cuda': torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(20): model.extract_features(dummy)\n",
        "    if device == 'cuda': torch.cuda.synchronize()\n",
        "    throughputs.append((bs * 20) / (time.time() - start))\n",
        "\n",
        "plt.bar([str(b) for b in batch_sizes], throughputs)\n",
        "plt.ylabel('Throughput (img/s)')\n",
        "plt.savefig('/kaggle/working/throughput.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. t-SNE Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract eval embeddings for t-SNE\n",
        "emb_list, lbl_list = [], []\n",
        "with torch.no_grad():\n",
        "    for img, identity in tqdm(eval_loader):\n",
        "        if len(emb_list) * 128 >= 2000: break\n",
        "        emb_list.append(model.extract_features(img.to(device)).cpu().numpy())\n",
        "        lbl_list.extend([identity_to_idx.get(i, -1) for i in identity])\n",
        "\n",
        "embs = np.concatenate(emb_list)[:2000]\n",
        "lbls = np.array(lbl_list)[:2000]\n",
        "valid = lbls >= 0\n",
        "embs, lbls = embs[valid], lbls[valid]\n",
        "\n",
        "# Top 50 classes\n",
        "uniq = np.unique(lbls)[:50]\n",
        "mask = np.isin(lbls, uniq)\n",
        "embs, lbls = embs[mask], lbls[mask]\n",
        "\n",
        "print(f\"t-SNE samples: {len(embs)}\")\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "embs_2d = tsne.fit_transform(embs)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(embs_2d[:, 0], embs_2d[:, 1], c=lbls, cmap='tab20', s=10, alpha=0.6)\n",
        "plt.title('t-SNE Embedding')\n",
        "plt.savefig('/kaggle/working/tsne.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Final Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'method': 'embedding-based (cosine similarity)',\n",
        "    'model': {\n",
        "        'num_classes': int(num_classes),\n",
        "        'epochs': int(checkpoint.get('epoch', 0) + 1),\n",
        "        'training_val_acc': float(checkpoint.get('best_val_acc', 0))\n",
        "    },\n",
        "    'metrics': {\n",
        "        'top1_accuracy': float(top1_acc),\n",
        "        'top5_accuracy': float(top5_acc),\n",
        "        'auc': float(roc_auc),\n",
        "        'eer': float(eer) if 'eer' in globals() else None,\n",
        "        'eer_threshold': float(eer_threshold) if 'eer_threshold' in globals() else None\n",
        "    },\n",
        "    'performance': {\n",
        "        'avg_latency_ms': float(np.mean(latencies)),\n",
        "        'max_throughput': float(max(throughputs))\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('/kaggle/working/evaluation_report.json', 'w') as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL EVALUATION REPORT\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Method: Embedding-based (cosine similarity)\")\n",
        "print(f\"Top-1 Accuracy: {top1_acc:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {top5_acc:.2f}%\")\n",
        "print(f\"AUC: {roc_auc:.4f}\")\n",
        "if 'eer' in globals():\n",
        "    print(f\"EER: {eer:.4f} (threshold: {eer_threshold:.3f})\")\n",
        "print(f\"Avg Latency: {np.mean(latencies):.2f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls -la /kaggle/working/*.png /kaggle/working/*.json"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
