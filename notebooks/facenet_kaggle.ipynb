{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceNet Training - Kaggle\n",
    "\n",
    "Notebook huấn luyện FaceNet trên Kaggle với GPU miễn phí.\n",
    "\n",
    "## Chuẩn bị:\n",
    "1. Upload dataset `CelebA_Aligned_Balanced` lên Kaggle Datasets\n",
    "2. Add dataset vào notebook này\n",
    "3. Bật GPU: Settings > Accelerator > GPU P100/T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect môi trường\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "print(f\"Kaggle environment: {IS_KAGGLE}\")\n",
    "\n",
    "if not IS_KAGGLE:\n",
    "    print(\"WARNING: Notebook này được thiết kế cho Kaggle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình đường dẫn Kaggle\n",
    "ROOT = \"/kaggle/working/FaceRecognition\"\n",
    "CHECKPOINT_DIR = \"/kaggle/working/checkpoints/facenet\"\n",
    "\n",
    "# Dataset path - thay đổi theo tên dataset của bạn trên Kaggle\n",
    "KAGGLE_DATASET_NAME = \"celeba-aligned-balanced\"\n",
    "DATA_DIR = f\"/kaggle/input/{KAGGLE_DATASET_NAME}\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ROOT: {ROOT}\")\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"CHECKPOINT_DIR: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CAU HINH CHECKPOINT DATASET ===\n",
    "CHECKPOINT_DATASET_NAME = \"\"\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "if CHECKPOINT_DATASET_NAME:\n",
    "    checkpoint_input_dir = f\"/kaggle/input/{CHECKPOINT_DATASET_NAME}\"\n",
    "    if os.path.exists(checkpoint_input_dir):\n",
    "        print(f\"[OK] Tim thay checkpoint dataset\")\n",
    "        pth_files = glob.glob(os.path.join(checkpoint_input_dir, \"**/*.pth\"), recursive=True)\n",
    "        if pth_files:\n",
    "            os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "            for pth_file in pth_files:\n",
    "                dest_path = os.path.join(CHECKPOINT_DIR, os.path.basename(pth_file))\n",
    "                if not os.path.exists(dest_path):\n",
    "                    shutil.copy(pth_file, dest_path)\n",
    "                    print(f\"[COPY] {os.path.basename(pth_file)}\")\n",
    "else:\n",
    "    print(\"[INFO] Training tu dau (khong co checkpoint)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra Kaggle dataset\n",
    "print(\"=== KAGGLE INPUT DATASETS ===\")\n",
    "!ls -la /kaggle/input/\n",
    "\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"\\n[OK] Dataset found at: {DATA_DIR}\")\n",
    "    !ls -la {DATA_DIR}\n",
    "else:\n",
    "    print(f\"\\n[ERROR] Dataset not found at: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cau hinh GitHub token\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "    print(\"[OK] Da lay GITHUB_TOKEN\")\n",
    "except Exception as e:\n",
    "    GITHUB_TOKEN = None\n",
    "    print(\"[INFO] Su dung public URL\")\n",
    "\n",
    "if GITHUB_TOKEN:\n",
    "    REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/sin0235/FaceRecognition.git\"\n",
    "else:\n",
    "    REPO_URL = \"https://github.com/sin0235/FaceRecognition.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "if os.path.exists(ROOT):\n",
    "    print(\"Repository da ton tai, dang pull updates...\")\n",
    "    %cd {ROOT}\n",
    "    if GITHUB_TOKEN:\n",
    "        !git remote set-url origin {REPO_URL}\n",
    "    !git pull\n",
    "else:\n",
    "    print(f\"Dang clone repository...\")\n",
    "    !git clone {REPO_URL} {ROOT}\n",
    "    %cd {ROOT}\n",
    "\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thêm ROOT vào Python path\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "    print(f\"Da them {ROOT} vao Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt dependencies (KHONG cai lai torch)\n",
    "print(\"Cai dat dependencies...\")\n",
    "!pip install -q facenet-pytorch --no-deps\n",
    "!pip install -q opencv-python-headless Pillow scikit-learn tqdm pyyaml\n",
    "print(\"\\nHoan tat cai dat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra GPU\n",
    "import torch\n",
    "\n",
    "print(\"=== GPU INFO ===\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra dữ liệu training\n",
    "train_img_dir = os.path.join(DATA_DIR, \"train\")\n",
    "val_img_dir = os.path.join(DATA_DIR, \"val\")\n",
    "\n",
    "if not os.path.exists(train_img_dir):\n",
    "    train_img_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"train\")\n",
    "    val_img_dir = os.path.join(DATA_DIR, \"CelebA_Aligned_Balanced\", \"val\")\n",
    "\n",
    "print(\"=== KIEM TRA DU LIEU ===\")\n",
    "\n",
    "if os.path.exists(train_img_dir):\n",
    "    train_identities = [d for d in os.listdir(train_img_dir) \n",
    "                        if os.path.isdir(os.path.join(train_img_dir, d))]\n",
    "    print(f\"[OK] Train: {len(train_identities)} identities\")\n",
    "else:\n",
    "    print(f\"[ERROR] Train folder not found\")\n",
    "\n",
    "if os.path.exists(val_img_dir):\n",
    "    val_identities = [d for d in os.listdir(val_img_dir) \n",
    "                      if os.path.isdir(os.path.join(val_img_dir, d))]\n",
    "    print(f\"[OK] Val: {len(val_identities)} identities\")\n",
    "else:\n",
    "    print(f\"[ERROR] Val folder not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training FaceNet với Hard Negative Mining\n",
    "\n",
    "Sử dụng script `train_facenet.py` với `--mining hard` để tự động:\n",
    "- Mine hard triplets mỗi batch dựa trên embeddings hiện tại\n",
    "- Tạo embeddings discriminative hơn (similarity giữa identities khác nhau thấp hơn)\n",
    "- Logging đầy đủ metrics (loss, accuracy, distance, GPU memory, epoch time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override config paths cho Kaggle\n",
    "import yaml\n",
    "\n",
    "# Tìm train/val directories\n",
    "if os.path.exists(os.path.join(DATA_DIR, 'train')):\n",
    "    data_root = DATA_DIR\n",
    "elif os.path.exists(os.path.join(DATA_DIR, 'CelebA_Aligned_Balanced', 'train')):\n",
    "    data_root = os.path.join(DATA_DIR, 'CelebA_Aligned_Balanced')\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Cannot find train/val folders in {DATA_DIR}\")\n",
    "\n",
    "print(f\"Data root: {data_root}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override config file paths cho Kaggle environment\n",
    "config_path = os.path.join(ROOT, 'configs/facenet_kaggle.yaml')\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update paths\n",
    "config['dataset']['train_data_root'] = os.path.join(data_root, 'train')\n",
    "config['dataset']['val_data_root'] = os.path.join(data_root, 'val')\n",
    "config['path'] = {\n",
    "    'checkpoint_dir': CHECKPOINT_DIR,\n",
    "    'logs_dir': os.path.join(CHECKPOINT_DIR, 'logs')\n",
    "}\n",
    "\n",
    "# Save temp config\n",
    "temp_config_path = '/kaggle/working/facenet_kaggle_temp.yaml'\n",
    "with open(temp_config_path, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(f\"[OK] Created temp config: {temp_config_path}\")\n",
    "print(f\"\\nConfig overview:\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  LR: {config['training']['learning_rate']}\")\n",
    "print(f\"  Margin: {config['model']['margin']}\")\n",
    "print(f\"  Scheduler step: {config['training']['scheduler_step']}\")\n",
    "print(f\"  Scheduler gamma: {config['training']['scheduler_gamma']}\")\n",
    "print(f\"  Patience: {config['training']['patience']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FaceNet với HARD NEGATIVE MINING\n",
    "print(\"=\"*60)\n",
    "print(\"BAT DAU TRAINING FACENET - HARD NEGATIVE MINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "!python models/facenet/train_facenet.py \\\n",
    "    --config {temp_config_path} \\\n",
    "    --mining hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "history_path = os.path.join(CHECKPOINT_DIR, 'logs', 'training_history.json')\n",
    "\n",
    "if os.path.exists(history_path):\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Triplet Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Triplet Accuracy (fast metric)\n",
    "    axes[0, 1].plot(history['train_acc'], label='Train')\n",
    "    axes[0, 1].plot(history.get('val_triplet_acc', []), label='Val')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Triplet Accuracy (d(a,p) < d(a,n))')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Verification Accuracy (realistic metric)\n",
    "    if 'val_ver_acc' in history and history['val_ver_acc']:\n",
    "        axes[0, 2].plot(history['val_ver_acc'], label='Verification Acc', color='green', linewidth=2)\n",
    "        axes[0, 2].set_xlabel('Epoch')\n",
    "        axes[0, 2].set_ylabel('Accuracy')\n",
    "        axes[0, 2].set_title('Verification Accuracy (Realistic)')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True)\n",
    "        axes[0, 2].set_ylim([0, 1])\n",
    "    else:\n",
    "        axes[0, 2].text(0.5, 0.5, 'No verification data', ha='center', va='center')\n",
    "        axes[0, 2].set_title('Verification Accuracy')\n",
    "    \n",
    "    # Positive Distance\n",
    "    axes[1, 0].plot(history['train_pos_dist'], label='Train')\n",
    "    axes[1, 0].plot(history['val_pos_dist'], label='Val')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Distance')\n",
    "    axes[1, 0].set_title('Positive Distance (same identity)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Negative Distance\n",
    "    axes[1, 1].plot(history['train_neg_dist'], label='Train')\n",
    "    axes[1, 1].plot(history['val_neg_dist'], label='Val')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Distance')\n",
    "    axes[1, 1].set_title('Negative Distance (different identity)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    # Verification Threshold\n",
    "    if 'val_ver_threshold' in history and history['val_ver_threshold']:\n",
    "        axes[1, 2].plot(history['val_ver_threshold'], label='Optimal Threshold', color='purple', linewidth=2)\n",
    "        axes[1, 2].set_xlabel('Epoch')\n",
    "        axes[1, 2].set_ylabel('Threshold')\n",
    "        axes[1, 2].set_title('Verification Threshold (Cosine Similarity)')\n",
    "        axes[1, 2].legend()\n",
    "        axes[1, 2].grid(True)\n",
    "        axes[1, 2].set_ylim([0, 1])\n",
    "    else:\n",
    "        axes[1, 2].text(0.5, 0.5, 'No threshold data', ha='center', va='center')\n",
    "        axes[1, 2].set_title('Verification Threshold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_curves.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mining strategy: {history.get('mining_strategy', 'N/A')}\")\n",
    "    print(f\"Total time: {history.get('total_time_minutes', 0):.2f} minutes\")\n",
    "    print(f\"Best val loss: {history.get('best_val_loss', 0):.4f}\")\n",
    "    \n",
    "    # Show final metrics\n",
    "    if history.get('val_triplet_acc'):\n",
    "        final_triplet_acc = history['val_triplet_acc'][-1]\n",
    "        print(f\"Final triplet acc: {final_triplet_acc:.4f} (fast metric)\")\n",
    "    \n",
    "    if history.get('val_ver_acc'):\n",
    "        final_ver_acc = history['val_ver_acc'][-1]\n",
    "        final_threshold = history.get('val_ver_threshold', [0.5])[-1]\n",
    "        print(f\"Final verification acc: {final_ver_acc:.4f} @ threshold={final_threshold:.2f} (realistic)\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(f\"[ERROR] History file not found: {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiển thị checkpoint files\n",
    "print(\"=== CHECKPOINT FILES ===\")\n",
    "!ls -lh {CHECKPOINT_DIR}\n",
    "\n",
    "print(\"\\n=== Download ===\")\n",
    "print(f\"Best model: {CHECKPOINT_DIR}/facenet_best.pth\")\n",
    "print(f\"Last model: {CHECKPOINT_DIR}/facenet_last.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip checkpoint folder de tai ve\n",
    "import shutil\n",
    "\n",
    "zip_name = \"facenet_checkpoints\"\n",
    "zip_path = f\"/kaggle/working/{zip_name}\"\n",
    "\n",
    "shutil.make_archive(zip_path, \"zip\", CHECKPOINT_DIR)\n",
    "\n",
    "print(f\"[OK] Da tao file zip: {zip_path}.zip\")\n",
    "print(f\"\\nDownload file nay tu panel Output ben phai.\")\n",
    "!ls -lh /kaggle/working/*.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}